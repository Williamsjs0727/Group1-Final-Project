{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7512a413",
   "metadata": {},
   "source": [
    "# NYC Apartment Search - Group 1\n",
    "\n",
    "### Purpose of the Project:\n",
    "The project uses data-driven approaches to analyze and visualize New York City apartment data, 311 complaints, and urban forestry data to help understand urban living dynamics. This analysis is intended to aid in making informed decisions about apartment rentals based on environmental and urban living conditions.\n",
    "\n",
    "### Sections and Key Functions:\n",
    "1. **Setup**\n",
    "   - Initializes the environment with necessary libraries and settings.\n",
    "\n",
    "2. **Part 1: Data Preprocessing**\n",
    "   - Functions to load and clean data from various sources (ZIP codes, 311 complaints, tree census, Zillow rent data).\n",
    "   - Quality checks and basic data explorations are conducted.\n",
    "\n",
    "3. **Part 2: Storing Data**\n",
    "   - Database setup functions to create tables and indices.\n",
    "   - Functions to convert geometries for database insertion and to insert cleaned data into a PostgreSQL database.\n",
    "   - Data retrieval functions to fetch and display samples from each database table.\n",
    "\n",
    "4. **Part 3: Understanding the Data**\n",
    "   - Functions to execute SQL queries and to extract meaningful insights from the database.\n",
    "   - Various SQL queries analyze the relationship between apartment prices, complaints, and tree census data.\n",
    "\n",
    "5. **Part 4: Visualizing the Data**\n",
    "   - Multiple visualizations to represent data insights graphically, including trends over time and spatial distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b027f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.base import Engine\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2 import Geometry, WKTElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d09e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"nyc_zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "QUERY_DIR = pathlib.Path(\"queries\")  # Directory for saving DB queries\n",
    "\n",
    "# API configuration\n",
    "APP_TOKEN = \"J9t5fS2TcfDISWng9WsnCdvCP\"\n",
    "COMPLAINTS_URL = 'https://data.cityofnewyork.us/resource/erm2-nwe9.geojson'\n",
    "TREES_URL = 'https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson'\n",
    "\n",
    "# Database configuration\n",
    "DB_NAME = \"nyc_data\"\n",
    "DB_USER = \"williamsjs\"\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory: pathlib.Path):\n",
    "    \"\"\"Ensure that a directory exists; if not, create it.\"\"\"\n",
    "    try:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {directory}: {e}\")\n",
    "\n",
    "# Make sure the directories exist\n",
    "ensure_directory_exists(DATA_DIR)\n",
    "ensure_directory_exists(QUERY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119d826",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "The first part of the data cleaning process involves the following steps:\n",
    "\n",
    "1. **Reading and cleaning the Zillow rental data**:\n",
    "   - Loading the Zillow data from a CSV file\n",
    "   - Melting the data so that each row represents a unique date-region pair\n",
    "   - Filtering the data to include only New York City and the relevant date range (February 2022 to January 2024)\n",
    "   - Keeping the required columns (zipcode, city, date, rent price) and renaming them\n",
    "   - Converting the \"zipcode\" column to a string type and the \"date\" column to a datetime type\n",
    "\n",
    "2. **Reading and cleaning the zipcode data**\n",
    "3. **Downloading, cleaning, and filtering the 311 complaints data and tree data**\n",
    "4. **Filtering all the datasets to include only the cleaned zipcode data**\n",
    "\n",
    "5. **Performing data quality checks**:\n",
    "   - Checking for null values in each dataset\n",
    "   - Checking for duplicate entries in each dataset\n",
    "   - Cross-referencing the zipcodes across the datasets to ensure consistency\n",
    "6. **Show information and first 5 entries of each dataset**.\n",
    "\n",
    "Overall, the purpose of this part of the code is to extract, clean, and integrate the necessary information from the original data sources, preparing the data for further analysis. It involves key steps such as data loading, data cleaning, data filtering, and data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90e596",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zipcode_data()` function \n",
    "reads in a shapefile containing zipcode data, cleans and preprocesses the data, and returns a GeoDataFrame with unique zipcodes and their corresponding geometries. \n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading the shapefile using Geopandas, a library for working with geospatial data.\n",
    "2. Selecting the relevant columns (zipcode and geometry) and renaming the 'ZIPCODE' column to 'zipcode' for better readability.\n",
    "3. Converting the coordinate reference system (CRS) of the GeoDataFrame to EPSG:4326 (WGS84) for consistency.\n",
    "4. Removing any duplicate zipcode entries, keeping only the first occurrence of each unique zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zipcode_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean zipcode data from a shapefile.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing unique zipcodes and their geometries.\n",
    "    \"\"\"\n",
    "    # Read the shapefile using Geopandas\n",
    "    zipcode_data = gpd.read_file(ZIPCODE_DATA_FILE)\n",
    "    \n",
    "    # Select relevant columns and rename them\n",
    "    zipcode_cleaned = zipcode_data[['ZIPCODE', 'geometry']].rename(columns={'ZIPCODE': 'zipcode'})\n",
    "    \n",
    "    # Convert CRS to EPSG:4326 for consistency\n",
    "    zipcode_cleaned = zipcode_cleaned.to_crs(epsg=4326)\n",
    "    \n",
    "    # Remove duplicate zipcodes, keeping the first occurrence\n",
    "    zipcode_cleaned = zipcode_cleaned.drop_duplicates(subset=['zipcode'], keep='first')\n",
    "    \n",
    "    return zipcode_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd245d",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_311_data()` function \n",
    "fetches 311 complaint data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned 311 complaints.\n",
    "\n",
    "The key steps are:\n",
    "1. Defining the API parameters to fetch 311 complaint data within a specific date range and with valid latitude/longitude coordinates.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and setting the appropriate coordinate reference system (EPSG:4326).\n",
    "4. Selecting and renaming the relevant columns, and dropping any rows with missing zipcodes.\n",
    "5. Converting the 'created_date' column from a string to a date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean 311 complaint data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing 311 complaints with relevant fields and valid zipcodes.\n",
    "    \"\"\"\n",
    "    # API parameters for fetching data\n",
    "    complaints_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$where': 'created_date >= \"2022-02-01T00:00:00.000\" AND created_date <= \"2024-02-29T00:00:00.000\" AND latitude IS NOT NULL',\n",
    "        '$limit': 1000000\n",
    "    }\n",
    "    \n",
    "    # Requesting data from the API\n",
    "    complaints_response = requests.get(COMPLAINTS_URL, params=complaints_params)\n",
    "    if complaints_response.status_code != 200:\n",
    "        raise Exception(\"Failed to download data\")\n",
    "\n",
    "    # Create a GeoDataFrame from the response\n",
    "    complaints_data = gpd.GeoDataFrame.from_features(complaints_response.json()['features']).set_crs(epsg=4326)\n",
    "    \n",
    "    # Select and rename columns, and drop rows without zipcodes\n",
    "    complaints_cleaned = complaints_data[['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'geometry']].rename(columns={\n",
    "        'unique_key': 'unique_id', \n",
    "        'incident_zip': 'zipcode'\n",
    "    }).dropna(subset=['zipcode'])\n",
    "    \n",
    "    # Convert 'created_date' from string to date\n",
    "    complaints_cleaned['created_date'] = pd.to_datetime(complaints_cleaned['created_date']).dt.date\n",
    "    \n",
    "    return complaints_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61566a",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_tree_data()` function \n",
    "fetches tree data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned tree data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Defining the API parameters to fetch tree data, with a limit of 10 million records.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and dropping any rows where the latitude or longitude is missing.\n",
    "4. Converting the latitude and longitude columns to float and creating geometry points from them.\n",
    "5. Selecting and renaming the relevant columns, and dropping any rows where critical information (zipcode, health, or species) is missing.\n",
    "6. Setting the coordinate reference system (EPSG:4326) for the GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean tree data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing tree data with geometries created from latitude and longitude.\n",
    "    \"\"\"\n",
    "    trees_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$limit': 10000000\n",
    "    }\n",
    "    \n",
    "    trees_response = requests.get(TREES_URL, params=trees_params)\n",
    "    if trees_response.status_code != 200:\n",
    "        print(f\"Failed to download tree data. Status code: {trees_response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Create a GeoDataFrame from the JSON response\n",
    "    trees_data = gpd.GeoDataFrame.from_features(trees_response.json())\n",
    "\n",
    "    # Drop rows where latitude or longitude is NaN, and convert them to float\n",
    "    trees_data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    trees_data['latitude'] = trees_data['latitude'].astype(float)\n",
    "    trees_data['longitude'] = trees_data['longitude'].astype(float)\n",
    "\n",
    "    # Create geometry points from latitude and longitude\n",
    "    trees_data['geometry'] = trees_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "    # Select and rename columns, drop rows where any critical information is missing\n",
    "    trees_cleaned = trees_data[['tree_id', 'spc_common', 'health', 'status', 'zipcode', 'geometry']].rename(\n",
    "        columns={'spc_common': 'species'}\n",
    "    )\n",
    "    trees_cleaned['zipcode'] = trees_cleaned['zipcode'].astype(str)\n",
    "    trees_cleaned.crs = 'EPSG:4326'\n",
    "    trees_cleaned.dropna(subset=['zipcode', 'health', 'species'], inplace=True)\n",
    "\n",
    "    return trees_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a0dd9",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zillow_data()` function \n",
    "reads Zillow rental data for New York City from a CSV file, cleans and preprocesses the data, and returns a cleaned DataFrame.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Loading the Zillow data from a CSV file into a DataFrame.\n",
    "2. Melting the data so that each row represents a unique date-region pair, making it easier to work with.\n",
    "3. Filtering the data to include only New York City and selecting the relevant date range (February 2022 to January 2024).\n",
    "4. Keeping only the required columns (zipcode, city, data_date, rent_price) and renaming them for better readability.\n",
    "5. Converting the 'zipcode' column to a string and the 'data_date' column to a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a126471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zillow_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean Zillow rental data for New York City.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A cleaned DataFrame containing Zillow rental data with selected columns and filtered dates.\n",
    "    \"\"\"\n",
    "    # Load Zillow data from a CSV file\n",
    "    zillow_data = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "    \n",
    "    # Melt the data so that every row is a unique date-region pair\n",
    "    id_vars = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName']\n",
    "    melted_data = pd.melt(zillow_data, id_vars=id_vars, var_name='data_date', value_name='rent_price')\n",
    "    \n",
    "    # Filter for New York City data and select relevant dates\n",
    "    zillow_cleaned = melted_data[\n",
    "        (melted_data['City'] == 'New York') &\n",
    "        (melted_data['data_date'] >= '2022-02-01') &\n",
    "        (melted_data['data_date'] <= '2024-01-31')\n",
    "    ]\n",
    "    \n",
    "    # Keep only the required columns and rename them\n",
    "    zillow_cleaned = zillow_cleaned[['RegionName', 'City', 'data_date', 'rent_price']].rename(\n",
    "        columns={'RegionName': 'zipcode', 'City': 'city', 'data_date': 'data_date', 'rent_price': 'rent_price'}\n",
    "    ).dropna()\n",
    "    \n",
    "    # Convert data types\n",
    "    zillow_cleaned['zipcode'] = zillow_cleaned['zipcode'].astype(str)\n",
    "    zillow_cleaned['data_date'] = pd.to_datetime(zillow_cleaned['data_date'])\n",
    "\n",
    "    return zillow_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d12aa",
   "metadata": {},
   "source": [
    "#### The `load_and_clean_all_data()` function \n",
    "is responsible for loading and cleaning multiple datasets, including zip codes, Zillow rental data, 311 complaints, and tree data. It then filters all the datasets to only include entries corresponding to the cleaned zip code data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading and cleaning the zip code data using the `read_and_clean_zipcode_data()` function.\n",
    "2. Reading, cleaning, and filtering the Zillow rental data using the `read_and_clean_zillow_data()` function, ensuring that only the data for the cleaned zip codes is retained.\n",
    "3. Downloading, cleaning, and filtering the 311 complaints data using the `download_and_clean_311_data()` function, again ensuring that only the data for the cleaned zip codes is kept.\n",
    "4. Downloading, cleaning, and filtering the tree data using the `download_and_clean_tree_data()` function, keeping only the data for the cleaned zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_all_data() -> Tuple[gpd.GeoDataFrame, pd.DataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean all relevant datasets including zip codes, rental data, 311 complaints, and tree data,\n",
    "    filtering them to only include entries corresponding to the cleaned zip code data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[GeoDataFrame, DataFrame, GeoDataFrame, GeoDataFrame]: A tuple containing the cleaned data for\n",
    "        zip codes, Zillow rental, 311 complaints, and tree data respectively.\n",
    "    \"\"\"\n",
    "    # Read and clean zipcode data\n",
    "    zipcode_data = read_and_clean_zipcode_data()\n",
    "    \n",
    "    # Read, clean and filter Zillow rental data\n",
    "    zillow_data = read_and_clean_zillow_data()\n",
    "    zillow_data = zillow_data[zillow_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter 311 complaints data\n",
    "    complaints_data = download_and_clean_311_data()\n",
    "    if complaints_data is not None:\n",
    "        complaints_data = complaints_data[complaints_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter tree data\n",
    "    tree_data = download_and_clean_tree_data()\n",
    "    if tree_data is not None:\n",
    "        tree_data = tree_data[tree_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "\n",
    "    return zipcode_data, zillow_data, complaints_data, tree_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a1a81",
   "metadata": {},
   "source": [
    "#### Load and clean all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean all data sets\n",
    "zipcode_data, zillow_data, complaints_data, tree_data = load_and_clean_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22600da1",
   "metadata": {},
   "source": [
    "#### The `check_data_quality()` function \n",
    "performs a series of data quality checks on the loaded datasets, including:\n",
    "\n",
    "1. Checking for null values in each dataset (zipcode_data, zillow_data, complaints_data, and tree_data).\n",
    "2. Checking for duplicate entries in each dataset, such as duplicate zipcodes, tree IDs, and unique IDs.\n",
    "3. Cross-referencing the zipcodes across the datasets to ensure that all zipcodes in the Zillow rental, 311 complaints, and tree data are present in the cleaned zipcode_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a52ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality():\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for null values in each dataset\n",
    "    print(\"Null values in zipcode_data:\", zipcode_data.isnull().sum())\n",
    "    print(\"Null values in zillow_data:\", zillow_data.isnull().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Null values in complaints_data:\", complaints_data.isnull().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Null values in tree_data:\", tree_data.isnull().sum())\n",
    "\n",
    "    # Check for duplicate entries\n",
    "    print(\"Duplicate zipcodes in zipcode_data:\", zipcode_data['zipcode'].duplicated().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Duplicate tree_ids in tree_data:\", tree_data['tree_id'].duplicated().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Duplicate unique_ids in complaints_data:\", complaints_data['unique_id'].duplicated().sum())\n",
    "    print(\"Duplicate zipcodes in zillow_data:\", zillow_data['zipcode'].duplicated().sum())\n",
    "\n",
    "    # Cross-reference zipcodes across datasets\n",
    "    if zillow_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all zillow_data zipcodes in zipcode_data?\", all(zillow_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if complaints_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all complaints_data zipcodes in zipcode_data?\", all(complaints_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if tree_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all tree_data zipcodes in zipcode_data?\", all(tree_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "\n",
    "# Run the data quality checks\n",
    "check_data_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d66b2",
   "metadata": {},
   "source": [
    "#### Show basic info about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038dfe3",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaf6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a02f4",
   "metadata": {},
   "source": [
    "#### Show basic info about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76679837",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa76d09",
   "metadata": {},
   "source": [
    "#### Show basic info about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd949aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8dddb2",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f51f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b134733",
   "metadata": {},
   "source": [
    "#### Show basic info about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d492bd1",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893335ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dd289",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data\n",
    "\n",
    "The second part of the data storage process involves the following steps:\n",
    "\n",
    "1. **Defining and Applying the Database Schema `(define_and_apply_schema())`**:\n",
    "   - Created four tables: zipcode_data, complaints_data, tree_data, and zillow_data\n",
    "   - Defined the table structure, including primary keys, foreign keys, and other constraints\n",
    "   - Created spatial indexes on the geometry fields to improve query performance\n",
    "   - Wrote the defined schema to an SQL file and executed it to apply the schema to the database\n",
    "\n",
    "2. **Converting Geometries for Database Insertion `(convert_geom())`**:\n",
    "   - Converted the geometry data in the GeoDataFrame to Well-Known Text (WKT) format for storage in the database\n",
    "\n",
    "3. **Inserting Data into the Database `(insert_data())`**:\n",
    "   - Used the pandas `to_sql()` method to insert the cleaned datasets into the corresponding database tables\n",
    "   - Specified the data types, ensuring the geometry data is stored correctly\n",
    "   - Handled any exceptions that may occur during the data insertion process\n",
    "\n",
    "4. **Fetching Data from the Database `(fetch_data())`**:\n",
    "   - Defined a generic function to use pandas' `read_sql_query()` to retrieve data from the database based on a provided SQL query\n",
    "   - Provided an example usage, querying the row counts for each of the tables and returning the results\n",
    "\n",
    "In summary, this part of the code completed the process of storing the cleaned data in a PostgreSQL database, and provided related read and write interfaces, laying the foundation for further data analysis and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f993af",
   "metadata": {},
   "source": [
    "#### The `create_database()` function \n",
    "\n",
    "performs the following key steps:\n",
    "\n",
    "1. Creates a new PostgreSQL database named 'nyc_data' using the `createdb` command.\n",
    "2. Adds the PostGIS extension to the newly created database using the `CREATE EXTENSION postgis;` SQL command.\n",
    "3. If the database creation and PostGIS extension addition are successful, it prints a success message. If there is an error, it prints the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \"\"\"\n",
    "    Creates a new PostgreSQL database named 'nyc_data' and adds PostGIS extension.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"createdb\", DB_NAME], check=True)\n",
    "        subprocess.run([\"psql\", \"--dbname\", DB_NAME, \"-c\", \"CREATE EXTENSION postgis;\"], check=True)\n",
    "        print(\"Database and PostGIS extension created successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to create database or PostGIS extension: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6f6e4",
   "metadata": {},
   "source": [
    "#### The `define_and_apply_schema()` function \n",
    "\n",
    "performs the following key steps:\n",
    "\n",
    "1. It defines the schema for the database, including the creation of four tables: `zipcode_data`, `complaints_data`, `tree_data`, and `zillow_data`. Each table has a specific set of columns and constraints, such as primary keys and foreign key relationships.\n",
    "2. The function also creates spatial indices on the geometry columns of the tables, which will improve the performance of spatial queries.\n",
    "3. The defined schema is written to a SQL file, and then executed using the `psql` command to apply the schema to the 'nyc_data' database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290161cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_apply_schema():\n",
    "    \"\"\"\n",
    "    Defines the schema for the database and applies it using SQL file execution.\n",
    "    \"\"\"\n",
    "    schema_sql = \"\"\"\n",
    "        -- Drop existing tables and indices if they exist\n",
    "        DROP INDEX IF EXISTS idx_zipcode_geom CASCADE;\n",
    "        DROP INDEX IF EXISTS idx_complaints_geom CASCADE;\n",
    "        DROP INDEX IF EXISTS idx_tree_geom CASCADE;\n",
    "        DROP TABLE IF EXISTS complaints_data CASCADE;\n",
    "        DROP TABLE IF EXISTS tree_data CASCADE;\n",
    "        DROP TABLE IF EXISTS zillow_data CASCADE;\n",
    "        DROP TABLE IF EXISTS zipcode_data CASCADE;\n",
    "\n",
    "        -- Create tables\n",
    "        CREATE TABLE zipcode_data (\n",
    "            zipcode TEXT PRIMARY KEY,\n",
    "            geom GEOMETRY\n",
    "        );\n",
    "\n",
    "        CREATE TABLE complaints_data (\n",
    "            unique_id BIGINT PRIMARY KEY,\n",
    "            created_date DATE,\n",
    "            complaint_type TEXT,\n",
    "            zipcode TEXT,\n",
    "            geom GEOMETRY,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE tree_data (\n",
    "            tree_id BIGINT PRIMARY KEY,\n",
    "            species TEXT,\n",
    "            health TEXT,\n",
    "            status TEXT,\n",
    "            zipcode TEXT,\n",
    "            geom GEOMETRY,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE zillow_data (\n",
    "            zipcode TEXT,\n",
    "            city TEXT,\n",
    "            data_date DATE,\n",
    "            rent_price NUMERIC,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode),\n",
    "            PRIMARY KEY (zipcode, data_date)\n",
    "        );\n",
    "\n",
    "        -- Create spatial indices\n",
    "        CREATE INDEX idx_zipcode_geom ON zipcode_data USING gist(geom);\n",
    "        CREATE INDEX idx_complaints_geom ON complaints_data USING gist(geom);\n",
    "        CREATE INDEX idx_tree_geom ON tree_data USING gist(geom);\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the SQL schema to a file\n",
    "    with open(\"schema.sql\", \"w\") as file:\n",
    "        file.write(schema_sql)\n",
    "    \n",
    "    # Execute the SQL schema file\n",
    "    try:\n",
    "        subprocess.run([\"psql\", \"-d\", DB_NAME, \"-f\", \"schema.sql\"], check=True)\n",
    "        print(\"Database schema applied successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to apply database schema: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    define_and_apply_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e44b85",
   "metadata": {},
   "source": [
    "#### The `convert_geom()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a GeoDataFrame as input, which contains a 'geometry' column with spatial data.\n",
    "2. The function checks if the 'geometry' column exists in the input DataFrame.\n",
    "3. If the 'geometry' column is present, it applies the `WKTElement()` function to each geometry object in the column, converting it to a Well-Known Text (WKT) representation.\n",
    "4. The function then renames the 'geometry' column to 'geom' and drops the original 'geometry' column, returning a new DataFrame with the converted geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806836f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_geom(data: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert geometries to WKTElement for database insertion.\n",
    "\n",
    "    Args:\n",
    "        data (GeoDataFrame): The geopandas DataFrame containing the 'geometry' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame with converted geometry to WKTElement and renamed to 'geom'.\n",
    "    \"\"\"\n",
    "    if 'geometry' in data.columns:\n",
    "        data['geom'] = data['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "        return data.drop(columns=['geometry'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ab9c2",
   "metadata": {},
   "source": [
    "#### The `insert_data()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a pandas DataFrame, the name of the target database table, and an optional dictionary of SQLAlchemy data types as input.\n",
    "2. The function uses the `to_sql()` method of the DataFrame to insert the data into the specified table in the database.\n",
    "3. The `if_exists='append'` parameter ensures that the data is appended to the table, rather than overwriting any existing data.\n",
    "4. The `index=False` parameter ensures that the DataFrame index is not included in the insert.\n",
    "5. The `dtype` parameter is used to specify the SQL column types for the data, which is particularly important for the 'geom' column containing the spatial data.\n",
    "6. If the data insertion is successful, it prints a success message. If there is an error, it prints the error message.\n",
    "7. The code calls the `insert_data()` function for each of the cleaned datasets (`zipcode_data`, `complaints_data`, `tree_data`, and `zillow_data`), using the `convert_geom()` function to convert the geometries to the appropriate format before insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baddfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(df: pd.DataFrame, table_name: str, dtype: dict = None) -> None:\n",
    "    \"\"\"\n",
    "    Inserts data into a database table.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to insert into the database.\n",
    "        table_name (str): The name of the target database table.\n",
    "        dtype (dict): A dictionary of SQLAlchemy types to specify SQL column types.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the data insertion fails, it raises an exception with the error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False, dtype=dtype)\n",
    "        print(f\"{table_name} inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert {table_name}: {str(e)}\")\n",
    "\n",
    "# use the above functions to insert datasets to database\n",
    "insert_data(convert_geom(zipcode_data), 'zipcode_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(convert_geom(complaints_data), 'complaints_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(convert_geom(tree_data), 'tree_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(zillow_data, 'zillow_data')\n",
    "\n",
    "print(\"Database setup and data insertion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00916a",
   "metadata": {},
   "source": [
    "#### The `fetch_data()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a SQL query and a database engine connection as input.\n",
    "2. The function uses the `pd.read_sql_query()` method to execute the provided SQL query and fetch the resulting data as a pandas DataFrame.\n",
    "3. If there is an error during the data fetch, the function prints the error message and returns an empty DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cce3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(query: str, engine: Engine) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches data from the database based on the provided SQL query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to be executed.\n",
    "        engine (Engine): The database engine connection used to execute the query.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the results of the SQL query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql_query(query, con=engine)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "\n",
    "# Example usage of the fetch_data function\n",
    "query = \"\"\"\n",
    "    SELECT 'zipcode_data' AS table_name, COUNT(*) AS total_rows FROM zipcode_data UNION ALL\n",
    "    SELECT 'complaints_data' AS table_name, COUNT(*) FROM complaints_data UNION ALL\n",
    "    SELECT 'tree_data' AS table_name, COUNT(*) FROM tree_data UNION ALL\n",
    "    SELECT 'zillow_data' AS table_name, COUNT(*) FROM zillow_data;\n",
    "\"\"\"\n",
    "row_counts = fetch_data(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a6806",
   "metadata": {},
   "source": [
    "#### Print the row counts for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e5983",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'zipcode_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_sample = fetch_data(\"SELECT * FROM zipcode_data LIMIT 5;\", engine)\n",
    "zipcode_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadf6bd",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'complaints_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_sample = fetch_data(\"SELECT * FROM complaints_data LIMIT 5;\", engine)\n",
    "complaints_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88b1c7",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'tree_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sample = fetch_data(\"SELECT * FROM tree_data LIMIT 5;\", engine)\n",
    "tree_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88bafc",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'zillow_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99657952",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_sample = fetch_data(\"SELECT * FROM zillow_data LIMIT 5;\", engine)\n",
    "zillow_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b272a93",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n",
    "\n",
    "1. `QUERY1`: This query calculates the average rent price for each zipcode in the `zillow_data` table as of the date '2024-01-31'.\n",
    "\n",
    "2. `QUERY2`: This query counts the number of trees for each zipcode in the `tree_data` table.\n",
    "\n",
    "3. `QUERY3`: This query counts the number of complaints for each zipcode in the `complaints_data` table where the `created_date` is between '2024-01-01' and '2024-01-31'.\n",
    "\n",
    "4. `QUERY4`: This query finds the correlation between rent, trees, and complaints by joining the results of the previous three queries. It focuses on the top and bottom 5 zipcodes by average rent as of '2024-01-31'.\n",
    "\n",
    "5. `QUERY5`: This query calculates the percentage change in rent prices for each zipcode in the `zillow_data` table between the dates '2023-01-31' and '2024-01-31'.\n",
    "\n",
    "6. `QUERY6`: This query calculates the percentage change in the number of trees for each zipcode in the `tree_data` table between the dates '2023-01-01' and '2024-01-01'.\n",
    "\n",
    "These queries provide insights into the relationships between rent, trees, and complaints in the given data, as well as the changes in rent prices and tree counts over a one-year period from 2023 to 2024. The results can be used to analyze urban development and quality of life factors within the specified timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eaa69",
   "metadata": {},
   "source": [
    "#### The `read_query()` function \n",
    "performs the following tasks:\n",
    "\n",
    "1. It takes an SQL query (as a string) and a database engine connection as input parameters.\n",
    "2. It attempts to execute the given SQL query using the provided database engine connection and returns the results as a pandas DataFrame.\n",
    "3. If an exception occurs during the query execution, the function prints an error message and returns an empty pandas DataFrame instead of raising the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(query: str, engine: Engine) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes an SQL query and returns the results as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "        engine (Engine): The database engine connection to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql(query, con=engine)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute query: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabeb03",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "1. It defines an SQL query, `QUERY1`, that retrieves the number of 311 complaints per zipcode in New York City for the period between March 2023 and February 2024. The query groups the complaints by zipcode and orders the results by the complaint count in descending order.\n",
    "\n",
    "2. It writes the `QUERY1` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY1` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `complaint_counts` variable.\n",
    "\n",
    "4. Finally, it returns the `complaint_counts` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22966492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Number of 311 complaints per zip code\n",
    "# Define the query for the number of 311 complaints per zip code\n",
    "QUERY1 = \"\"\"\n",
    "    SELECT zipcode, COUNT(*) as complaint_count\n",
    "    FROM complaints_data\n",
    "    WHERE created_date >= '2023-03-01' AND created_date <= '2024-02-29'\n",
    "    GROUP BY zipcode\n",
    "    ORDER BY complaint_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file = QUERY_DIR / \"number_of_complaints_per_zipcode.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file, \"w\") as file:\n",
    "    file.write(QUERY1)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "complaint_counts = read_query(QUERY1, engine)\n",
    "complaint_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd7ea7",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "1. It defines an SQL query, `QUERY2`, that retrieves the top 10 zipcodes by tree count. The query groups the tree data by zipcode, counts the number of trees for each zipcode, orders the results by the tree count in descending order, and limits the output to the top 10 rows.\n",
    "\n",
    "2. It writes the `QUERY2` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY2` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `top_tree_counts` variable.\n",
    "\n",
    "4. Finally, it returns the `top_tree_counts` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2: Top 10 zip codes by tree count\n",
    "# Define the query for the top 10 zip codes by tree count\n",
    "QUERY2 = \"\"\"\n",
    "    SELECT zipcode, COUNT(*) as tree_count\n",
    "    FROM tree_data\n",
    "    GROUP BY zipcode\n",
    "    ORDER BY tree_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file2 = QUERY_DIR / \"top_10_zipcodes_by_tree_count.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file2, \"w\") as file:\n",
    "    file.write(QUERY2)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "top_tree_counts = read_query(QUERY2, engine)\n",
    "top_tree_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc1f8c",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "1. It defines an SQL query, `QUERY3`, that retrieves the average rent in the top 10 zipcodes with the most trees. The query uses a common table expression (CTE) to first identify the top 10 zipcodes by tree count, and then joins this with the Zillow rental data to calculate the average rent for each of those top 10 zipcodes.\n",
    "\n",
    "2. It writes the `QUERY3` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY3` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `average_rent_in_greenest_areas` variable.\n",
    "\n",
    "4. Finally, it returns the `average_rent_in_greenest_areas` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 3: Average rent in zip codes with the most trees\n",
    "# Define the query for the average rent in zip codes with the most trees\n",
    "QUERY3 = \"\"\"\n",
    "    WITH TopTreeZipCodes AS (\n",
    "        SELECT zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data\n",
    "        GROUP BY zipcode\n",
    "        ORDER BY tree_count DESC\n",
    "        LIMIT 10\n",
    "    )\n",
    "    SELECT t.zipcode, TO_CHAR(AVG(z.rent_price), 'FM9,999,999.00') as average_rent\n",
    "    FROM TopTreeZipCodes t\n",
    "    JOIN zillow_data z ON t.zipcode = z.zipcode AND z.data_date = '2024-01-31'\n",
    "    GROUP BY t.zipcode, t.tree_count\n",
    "    ORDER BY t.tree_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file3 = QUERY_DIR / \"average_rent_in_greenest_areas.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file3, \"w\") as file:\n",
    "    file.write(QUERY3)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "average_rent_in_greenest_areas = read_query(QUERY3, engine)\n",
    "average_rent_in_greenest_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec1f28",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "1. It defines an SQL query, `QUERY4`, that retrieves the correlation between rent, trees, and complaints for the top and bottom 5 zipcodes by average rent. The query uses three common table expressions (CTEs) to:\n",
    "   - Calculate the average rent and rent ranking for each zipcode\n",
    "   - Count the number of trees for each zipcode\n",
    "   - Count the number of complaints for each zipcode in January 2024\n",
    "\n",
    "2. It joins the results of these three CTEs to create a final result set that includes the zipcode, average rent, tree count, and complaint count for the top and bottom 5 zipcodes by average rent.\n",
    "\n",
    "3. It writes the `QUERY4` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "4. It executes the `QUERY4` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `correlation_results` variable.\n",
    "\n",
    "5. Finally, it returns the `correlation_results` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Rent, trees, and complaints correlation\n",
    "# Define the query for correlating rent, trees, and complaints\n",
    "QUERY4 = \"\"\"\n",
    "    WITH RentRank AS (\n",
    "        SELECT zipcode, AVG(rent_price) as average_rent,\n",
    "        RANK() OVER (ORDER BY AVG(rent_price) DESC) rent_rank_desc,\n",
    "        RANK() OVER (ORDER BY AVG(rent_price) ASC) rent_rank_asc\n",
    "        FROM zillow_data\n",
    "        WHERE data_date = '2024-01-31'\n",
    "        GROUP BY zipcode\n",
    "    ),\n",
    "    TreeCount AS (\n",
    "        SELECT zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data\n",
    "        GROUP BY zipcode\n",
    "    ),\n",
    "    ComplaintCount AS (\n",
    "        SELECT zipcode, COUNT(*) as complaint_count\n",
    "        FROM complaints_data\n",
    "        WHERE created_date >= '2024-01-01' AND created_date < '2024-02-01'\n",
    "        GROUP BY zipcode\n",
    "    )\n",
    "    SELECT r.zipcode,\n",
    "    TO_CHAR(r.average_rent, 'FM9,999,999.00') as average_rent,\n",
    "    t.tree_count,\n",
    "    c.complaint_count\n",
    "    FROM RentRank r\n",
    "    JOIN TreeCount t ON r.zipcode = t.zipcode\n",
    "    LEFT JOIN ComplaintCount c ON r.zipcode = c.zipcode\n",
    "    WHERE r.rent_rank_desc <= 5 OR r.rent_rank_asc <= 5\n",
    "    ORDER BY r.average_rent DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file4 = QUERY_DIR / \"rent_trees_complaints_correlation.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file4, \"w\") as file:\n",
    "    file.write(QUERY4)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "correlation_results = read_query(QUERY4, engine)\n",
    "correlation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829b86f",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "1. It defines an SQL query, `QUERY5`, that retrieves the top 10 zipcodes with the most greenery (trees) using a spatial join. The query uses a common table expression (CTE) to:\n",
    "   - Join the `tree_data` and `zipcode_data` tables using a spatial containment operation (`ST_Contains`) to determine which trees belong to each zipcode.\n",
    "   - Count the number of trees for each zipcode.\n",
    "\n",
    "2. It writes the `QUERY5` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY5` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `most_greenery_results` variable.\n",
    "\n",
    "4. Finally, it returns the `most_greenery_results` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Most greenery (take 2) using spatial join\n",
    "# Define the query to find the zip codes with the most greenery using a spatial join\n",
    "QUERY5 = \"\"\"\n",
    "    WITH TreeCount AS (\n",
    "        SELECT z.zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data t\n",
    "        JOIN zipcode_data z ON ST_Contains(z.geom, t.geom)\n",
    "        GROUP BY z.zipcode\n",
    "    )\n",
    "    SELECT zipcode, tree_count\n",
    "    FROM TreeCount\n",
    "    ORDER BY tree_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file5 = QUERY_DIR / \"most_greenery_spatial.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file5, \"w\") as file:\n",
    "    file.write(QUERY5)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "most_greenery_results = read_query(QUERY5, engine)\n",
    "most_greenery_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31864e4",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "1. It defines a central point using the `Point` class from the `shapely.geometry` module, with the coordinates (-73.96253174434912, 40.80737875669467).\n",
    "\n",
    "2. It creates a buffer around the central point with a radius of 0.5 miles (approximately 804.672 meters) using the `buffer()` method of the `Point` object. This creates a circular area around the central point.\n",
    "\n",
    "3. It constructs the SQL query `QUERY6` that retrieves the tree data (tree_id, species, health, status, and location) for all the trees within the buffered area. The `ST_DWithin()` function is used to check if the tree's geometry (`geom`) is within the buffered area.\n",
    "\n",
    "4. It writes the `QUERY6` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "5. It executes the `QUERY6` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `trees_within_half_mile` variable.\n",
    "\n",
    "6. Finally, it returns the `trees_within_half_mile` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec47e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 6: Trees within ½ mile radius\n",
    "# Define the central point\n",
    "central_point = Point(-73.96253174434912, 40.80737875669467)\n",
    "\n",
    "# Buffer the central point by 0.5 miles (approximately 804.672 meters)\n",
    "buffered_point = central_point.buffer(0.5 / 69)  # simple degree approximation\n",
    "\n",
    "# Construct the query using the buffered area\n",
    "QUERY6 = f\"\"\"\n",
    "    SELECT tree_id, species, health, status, ST_AsText(geom) as location\n",
    "    FROM tree_data\n",
    "    WHERE ST_DWithin(geom, ST_GeomFromText('{buffered_point.wkt}', 4326), 804.672);\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file6 = QUERY_DIR / \"trees_within_half_mile.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file6, \"w\") as file:\n",
    "    file.write(QUERY6)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "trees_within_half_mile = read_query(QUERY6, engine)\n",
    "trees_within_half_mile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5778a0",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "1. **Visualization 1: 311 Incident Trends over Time**\n",
    "   - This visualization shows the time trends of 311 service requests in New York City, including the monthly total number of complaints and the changing proportions of different complaint types. This helps to understand the seasonal patterns in 311 service demand and the changes in complaint types.\n",
    "\n",
    "\n",
    "2. **Visualization 2: Top 311 Incident Types**\n",
    "   - This visualization shows the most common types of 311 service requests and their respective proportions. This helps to identify the areas of greatest concern for New York City residents.\n",
    "\n",
    "3. **Visualization 3: 311 Incident Response Times**\n",
    "   - This visualization shows the distribution of 311 service request response times, including the average response time and the percentile values. This helps to evaluate the efficiency and responsiveness of the 311 service.\n",
    "\n",
    "4. **Visualization 4: 311 Incident Geospatial Heatmap**\n",
    "   - This visualization shows the geographical distribution of 311 service requests in New York City using a heatmap. This helps to identify the hotspot areas for 311 service demand.\n",
    "\n",
    "5. **Visualization 5: Geospatial Plot of 311 Incidents**\n",
    "   - This visualization shows the geographical distribution of 311 service requests within a 1-kilometer radius of the New York City center, using a scatter plot. This helps to gain a more detailed understanding of the 311 service demand in a specific area.\n",
    "\n",
    "6. **Visualization 6: Tree Data and New Tree Requests**\n",
    "   - This visualization shows the distribution of existing trees and new tree requests in New York City, using a scatter plot. This helps to understand the city's tree canopy coverage and the demand for new tree planting.\n",
    "\n",
    "Overall, these 6 visualizations cover various aspects of 311 service requests, including time trends, type distribution, response efficiency, and geographical distribution, providing valuable insights for analyzing and improving the 311 service. These visualization tools can help city management departments, policymakers, and community organizations better understand and meet the needs of the citizens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe6b8f",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code first runs a query to find the top 3 complaint types based on the total count of complaints between '2023-03-01' and '2024-02-29'.\n",
    "   - It then runs a second query to get the daily count of complaints for each of the top 3 complaint types during the same date range.\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The code checks if the query results are empty, and if so, it prints a message and returns.\n",
    "   - It then converts the 'created_date' column to a datetime format.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The code creates a line plot with the daily counts of the top 3 complaint types over time.\n",
    "   - The plot has a title, x-axis label (Date), y-axis label (Number of Complaints), and a legend to identify the different complaint types.\n",
    "   - The x-axis tick labels are rotated 45 degrees to improve readability.\n",
    "   - The plot is displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Complaint Types Over Time\n",
    "def plot_top_complaints_over_time(engine: Engine):\n",
    "    # Query to find top 3 complaint types\n",
    "    top_complaints_query = \"\"\"\n",
    "        SELECT complaint_type, COUNT(*) as count\n",
    "        FROM complaints_data\n",
    "        WHERE created_date >= '2023-03-01' AND created_date <= '2024-02-29'\n",
    "        GROUP BY complaint_type\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 3;\n",
    "    \"\"\"\n",
    "    top_complaints = read_query(top_complaints_query, engine)\n",
    "    \n",
    "    if top_complaints.empty:\n",
    "        print(\"No data found for top complaints.\")\n",
    "        return\n",
    "\n",
    "    # Prepare the tuple for SQL IN condition\n",
    "    top_types_tuple = tuple(top_complaints['complaint_type'].tolist()) if len(top_complaints) > 1 else (top_complaints['complaint_type'].iloc[0],)\n",
    "    \n",
    "    # Query to get daily counts for these top complaint types\n",
    "    complaint_trends_query = f\"\"\"\n",
    "        SELECT created_date, complaint_type, COUNT(*) as daily_count\n",
    "        FROM complaints_data\n",
    "        WHERE complaint_type IN {top_types_tuple} AND\n",
    "              created_date >= '2023-03-01' AND created_date <= '2024-02-29'\n",
    "        GROUP BY created_date, complaint_type\n",
    "        ORDER BY created_date;\n",
    "    \"\"\"\n",
    "    complaint_trends = read_query(complaint_trends_query, engine)\n",
    "    \n",
    "    if complaint_trends.empty:\n",
    "        print(\"No trend data found.\")\n",
    "        return\n",
    "\n",
    "    complaint_trends['date'] = pd.to_datetime(complaint_trends['created_date'])\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for ctype in top_complaints['complaint_type']:\n",
    "        data = complaint_trends[complaint_trends['complaint_type'] == ctype]\n",
    "        ax.plot(data['date'], data['daily_count'], label=ctype)\n",
    "    \n",
    "    ax.set_title('Daily Counts of Top 3 Complaint Types')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Number of Complaints')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_complaints_over_time(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e5938",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code executes a SQL query to get the top 10 complaint types in the zipcode '10027' between '2022-03-01' and '2024-02-29'.\n",
    "   - The query counts the number of complaints for each complaint type and orders the results in descending order, limiting the output to the top 10.\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The function checks if the resulting dataframe is empty, and if so, it prints a message and returns.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The code creates a bar plot using Matplotlib to visualize the top 10 complaint types in zipcode '10027'.\n",
    "   - The plot has a title, x-axis label (Complaint Type), y-axis label (Number of Complaints), and the x-axis tick labels are rotated 45 degrees for better readability.\n",
    "   - The plot is displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b42e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Complaints in Zip Code 10027\n",
    "def plot_complaints_in_zip(engine: Engine):\n",
    "    \"\"\"\n",
    "    Plots the top 10 complaint types in Zip Code 10027.\n",
    "    \n",
    "    Args:\n",
    "        engine (Engine): The database engine connection used for querying data.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT complaint_type, COUNT(*) as count\n",
    "        FROM complaints_data\n",
    "        WHERE zipcode = '10027' AND\n",
    "              created_date >= '2022-03-01' AND created_date <= '2024-02-29'\n",
    "        GROUP BY complaint_type\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\"\n",
    "    df = read_query(query, engine)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    # Plotting\n",
    "    ax = df.plot(kind='bar', x='complaint_type', y='count', legend=None, figsize=(10, 6), color='skyblue')\n",
    "    plt.title('Top 10 Complaint Types in Zip Code 10027')\n",
    "    plt.xlabel('Complaint Type')\n",
    "    plt.ylabel('Number of Complaints')\n",
    "    ax.set_xticklabels(df['complaint_type'], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_complaints_in_zip(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6399c1",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code defines three separate functions to fetch data from the database:\n",
    "     - `fetch_rent_data`: Retrieves the average rent price for each zipcode between '2022-02-01' and '2024-01-31'.\n",
    "     - `fetch_tree_data`: Retrieves the count of trees for each zipcode.\n",
    "     - `fetch_complaint_data`: Retrieves the count of complaints for each zipcode between '2022-02-01' and '2024-01-31'.\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The `plot_rent_trees_complaints_correlation` function calls the three data fetching functions and merges the resulting dataframes on the zipcode column.\n",
    "   - If any of the dataframes are empty, the function prints a message and returns.\n",
    "   - The function fills any NaN values in the merged dataframe with 0, as some zipcodes may not have data for one or more categories.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The function creates two subplots using Matplotlib, one for the correlation between average rent and tree count, and another for the correlation between average rent and complaint count.\n",
    "   - The first subplot uses a scatter plot to show the relationship between average rent and tree count.\n",
    "   - The second subplot uses a scatter plot to show the relationship between average rent and complaint count.\n",
    "   - Both subplots have appropriate titles, x-axis labels (Average Rent), and y-axis labels (Tree Count and Complaint Count, respectively).\n",
    "   - The plots are displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f76b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Correlation Between Rent, Trees, and Complaints\n",
    "def fetch_rent_data(engine: Engine):\n",
    "    rent_query = \"\"\"\n",
    "        SELECT zipcode, AVG(rent_price) as average_rent\n",
    "        FROM zillow_data\n",
    "        WHERE data_date BETWEEN '2022-02-01' AND '2024-01-31'\n",
    "        GROUP BY zipcode;\n",
    "    \"\"\"\n",
    "    return read_query(rent_query, engine)\n",
    "\n",
    "def fetch_tree_data(engine: Engine):\n",
    "    tree_query = \"\"\"\n",
    "        SELECT zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data\n",
    "        GROUP BY zipcode;\n",
    "    \"\"\"\n",
    "    return read_query(tree_query, engine)\n",
    "\n",
    "def fetch_complaint_data(engine: Engine):\n",
    "    complaint_query = \"\"\"\n",
    "        SELECT zipcode, COUNT(*) as complaint_count\n",
    "        FROM complaints_data\n",
    "        WHERE created_date BETWEEN '2022-02-01' AND '2024-01-31'\n",
    "        GROUP BY zipcode;\n",
    "    \"\"\"\n",
    "    return read_query(complaint_query, engine)\n",
    "\n",
    "def plot_rent_trees_complaints_correlation(engine: Engine):\n",
    "    df_rent = fetch_rent_data(engine)\n",
    "    df_trees = fetch_tree_data(engine)\n",
    "    df_complaints = fetch_complaint_data(engine)\n",
    "\n",
    "    if df_rent.empty or df_trees.empty or df_complaints.empty:\n",
    "        print(\"One or more data sets are empty. Cannot plot correlation.\")\n",
    "        return\n",
    "\n",
    "    # Merge dataframes on zipcode\n",
    "    df = pd.merge(pd.merge(df_rent, df_trees, on='zipcode', how='outer'), df_complaints, on='zipcode', how='outer')\n",
    "    df.fillna(0, inplace=True)  # Fill NaN values with 0 for zip codes with no data in one or more categories\n",
    "\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "    \n",
    "    # Plot Rent vs. Tree Count\n",
    "    ax1.scatter(df['average_rent'], df['tree_count'], color='green')\n",
    "    ax1.set_title('Average Rent vs. Tree Count')\n",
    "    ax1.set_ylabel('Tree Count')\n",
    "\n",
    "    # Plot Rent vs. Complaint Count\n",
    "    ax2.scatter(df['average_rent'], df['complaint_count'], color='red')\n",
    "    ax2.set_title('Average Rent vs. Complaint Count')\n",
    "    ax2.set_xlabel('Average Rent ($)')\n",
    "    ax2.set_ylabel('Complaint Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rent_trees_complaints_correlation(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fc9aa",
   "metadata": {},
   "source": [
    "### Visualization 4\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code defines a function `plot_rent_complaints_boxplot` that executes a SQL query to retrieve the following data:\n",
    "     - The rent bin (in $1,000 increments) for each zipcode, based on the latest rent data (as of '2024-01-31').\n",
    "     - The count of 311 complaints for each zipcode, where the complaints were created between '2023-02-01' and '2024-01-31'.\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The function checks if the resulting dataframe is empty, and if so, it prints a message and returns.\n",
    "   - It then converts the 'rent_bin' column to an integer type to ensure proper axis labeling.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The function creates a boxplot using Seaborn to visualize the distribution of 311 complaint counts for each rent bin.\n",
    "   - The plot has a title, 'Complaints by Rent Bins ($1000 increments)', and appropriate x-axis label ('Average Rent Bin ($)') and y-axis label ('Number of 311 Complaints').\n",
    "   - The x-axis tick labels are formatted to display the rent bin values in a more readable format (e.g., '$1,000', '$2,000', etc.) and rotated 45 degrees.\n",
    "   - The plot is displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe73d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization 4: Rent vs Complaints by Rent Bins\n",
    "def plot_rent_complaints_boxplot(engine: Engine):\n",
    "    \"\"\"\n",
    "    Creates a boxplot showing the distribution of 311 complaints by different rent bins.\n",
    "    \n",
    "    Args:\n",
    "        engine (Engine): The database engine connection used for querying data.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT floor(z.rent_price/1000)*1000 as rent_bin, COUNT(c.unique_id) as complaint_count\n",
    "        FROM (SELECT * FROM zillow_data WHERE data_date = '2024-01-31') z\n",
    "        JOIN complaints_data c ON z.zipcode = c.zipcode\n",
    "        WHERE c.created_date BETWEEN '2023-02-01' AND '2024-01-31'\n",
    "        GROUP BY z.zipcode, rent_bin\n",
    "    \"\"\"\n",
    "    df = read_query(query, engine)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "    \n",
    "    df['rent_bin'] = df['rent_bin'].astype(int)  # Ensure 'rent_bin' is an integer for better axis labeling\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.boxplot(x='rent_bin', y='complaint_count', data=df, color='skyblue')\n",
    "    ax.set_title('Complaints by Rent Bins ($1000 increments)')\n",
    "    ax.set_xlabel('Average Rent Bin ($)')\n",
    "    ax.set_ylabel('Number of 311 Complaints')\n",
    "    ax.set_xticklabels([f'${int(x.get_text()):,}' for x in ax.get_xticklabels()], rotation=45)  # Improve label format\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rent_complaints_boxplot(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557bb42",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code defines a function `plot_geospatial_incidents` that executes a SQL query to retrieve the following data:\n",
    "     - The complaint type, longitude, and latitude for 311 incidents that were created between '2023-03-01' and '2024-02-29', and are within a 1 km radius of a central point (specified in the query).\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The function checks if the resulting dataframe is empty, and if so, it prints a message and returns.\n",
    "   - It then converts the dataframe to a GeoDataFrame, which is a special type of DataFrame that includes a geometry column for spatial data.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The function creates a new figure and axis using Matplotlib.\n",
    "   - It then plots the 311 incident locations (represented as points) on the map using the `plot()` method of the GeoDataFrame.\n",
    "   - The plot has a title, '311 Incidents within 1 km Radius of Central Point', and appropriate x-axis label ('Longitude') and y-axis label ('Latitude').\n",
    "   - The plot is displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620816fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Geospatial Plot of 311 Incidents\n",
    "def plot_geospatial_incidents(engine: Engine):\n",
    "    \"\"\"\n",
    "    Plots 311 incidents within a 1 km radius of a central point using geospatial data.\n",
    "\n",
    "    Args:\n",
    "        engine (Engine): The database engine connection used for querying data.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT complaint_type, ST_X(geom) as longitude, ST_Y(geom) as latitude\n",
    "    FROM complaints_data\n",
    "    WHERE created_date BETWEEN '2023-03-01' AND '2024-02-29' AND ST_DWithin(geom, ST_SetSRID(ST_MakePoint(-73.96253174434912, 40.80737875669467), 4326), 1000)\n",
    "    \"\"\"\n",
    "    df = read_query(query, engine)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    gdf.plot(ax=ax, marker='o', color='blue', markersize=5)\n",
    "    ax.set_title('311 Incidents within 1 km Radius of Central Point')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.show()\n",
    "\n",
    "plot_geospatial_incidents(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c69180",
   "metadata": {},
   "source": [
    "### Visualization 6\n",
    "\n",
    "1. **Querying the Data**:\n",
    "   - The code defines a function `plot_trees_and_requests` that executes two SQL queries to retrieve the following data:\n",
    "     - The longitude and latitude for existing trees in the tree_data table.\n",
    "     - The longitude and latitude for new tree requests in the complaints_data table, where the complaint_type is 'New Tree Request' and the request was created between '2023-03-01' and '2024-02-29'.\n",
    "\n",
    "2. **Preparing the Data**:\n",
    "   - The function checks if the resulting dataframes for trees and requests are empty, and if so, it prints a message and returns.\n",
    "   - It then converts the dataframes to GeoDataFrames, which are special types of DataFrames that include a geometry column for spatial data.\n",
    "\n",
    "3. **Visualizing the Data**:\n",
    "   - The function creates a new figure and axis using Matplotlib.\n",
    "   - It then plots the existing trees and new tree requests on the map using the `plot()` method of the GeoDataFrames.\n",
    "   - The trees are represented as green circles, and the new tree requests are represented as red 'x' markers.\n",
    "   - The plot has a title, 'Trees and New Tree Requests in NYC', and appropriate x-axis label ('Longitude') and y-axis label ('Latitude').\n",
    "   - The plot includes a legend to distinguish between the existing trees and new tree requests.\n",
    "   - The plot is displayed using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Tree Data and New Tree Requests\n",
    "def plot_trees_and_requests(engine: Engine):\n",
    "    \"\"\"\n",
    "    Plots existing trees and new tree requests in New York City on a geospatial map.\n",
    "    \n",
    "    Args:\n",
    "        engine (Engine): The database engine connection used for querying data.\n",
    "    \"\"\"\n",
    "    tree_query = \"\"\"\n",
    "    SELECT ST_X(geom) as longitude, ST_Y(geom) as latitude\n",
    "    FROM tree_data\n",
    "    \"\"\"\n",
    "    df_t = read_query(tree_query, engine)\n",
    "    if df_t.empty:\n",
    "        print(\"No tree data available to plot.\")\n",
    "        return\n",
    "    trees = gpd.GeoDataFrame(df_t, geometry=gpd.points_from_xy(df_t['longitude'], df_t['latitude']))\n",
    "\n",
    "    request_query = \"\"\"\n",
    "    SELECT ST_X(geom) as longitude, ST_Y(geom) as latitude\n",
    "    FROM complaints_data\n",
    "    WHERE complaint_type = 'New Tree Request' AND created_date BETWEEN '2023-03-01' AND '2024-02-29'\n",
    "    \"\"\"\n",
    "    df_r = read_query(request_query, engine)\n",
    "    if df_r.empty:\n",
    "        print(\"No request data available to plot.\")\n",
    "        return\n",
    "    requests = gpd.GeoDataFrame(df_r, geometry=gpd.points_from_xy(df_r['longitude'], df_r['latitude']))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    trees.plot(ax=ax, marker='o', color='green', markersize=5, label='Existing Trees')\n",
    "    requests.plot(ax=ax, marker='x', color='red', markersize=5, label='New Tree Requests')\n",
    "    ax.set_title('Trees and New Tree Requests in NYC')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_trees_and_requests(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
