{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7512a413",
   "metadata": {},
   "source": [
    "# NYC Apartment Search - Group 1\n",
    "\n",
    "### Purpose of the Project:\n",
    "The project uses data-driven approaches to analyze and visualize New York City apartment data, 311 complaints, and urban forestry data to help understand urban living dynamics. This analysis is intended to aid in making informed decisions about apartment rentals based on environmental and urban living conditions.\n",
    "\n",
    "### Sections and Key Functions:\n",
    "1. **Setup**\n",
    "   - Initializes the environment with necessary libraries and settings.\n",
    "\n",
    "2. **Part 1: Data Preprocessing**\n",
    "   - Functions to load and clean data from various sources (ZIP codes, 311 complaints, tree census, Zillow rent data).\n",
    "   - Quality checks and basic data explorations are conducted.\n",
    "\n",
    "3. **Part 2: Storing Data**\n",
    "   - Database setup functions to create tables and indices.\n",
    "   - Functions to convert geometries for database insertion and to insert cleaned data into a PostgreSQL database.\n",
    "   - Data retrieval functions to fetch and display samples from each database table.\n",
    "\n",
    "4. **Part 3: Understanding the Data**\n",
    "   - Functions to execute SQL queries and to extract meaningful insights from the database.\n",
    "   - Various SQL queries analyze the relationship between apartment prices, complaints, and tree census data.\n",
    "\n",
    "5. **Part 4: Visualizing the Data**\n",
    "   - Multiple visualizations to represent data insights graphically, including trends over time and spatial distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b027f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.base import Engine\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2 import Geometry, WKTElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d09e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"nyc_zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "QUERY_DIR = pathlib.Path(\"queries\")  # Directory for saving DB queries\n",
    "\n",
    "# API configuration\n",
    "APP_TOKEN = \"J9t5fS2TcfDISWng9WsnCdvCP\"\n",
    "COMPLAINTS_URL = 'https://data.cityofnewyork.us/resource/erm2-nwe9.geojson'\n",
    "TREES_URL = 'https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson'\n",
    "\n",
    "# Database configuration\n",
    "DB_NAME = \"nyc_data\"\n",
    "DB_USER = \"williamsjs\"\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory: pathlib.Path):\n",
    "    \"\"\"Ensure that a directory exists; if not, create it.\"\"\"\n",
    "    try:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {directory}: {e}\")\n",
    "\n",
    "# Make sure the directories exist\n",
    "ensure_directory_exists(DATA_DIR)\n",
    "ensure_directory_exists(QUERY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119d826",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "The first part of the data cleaning process involves the following steps:\n",
    "\n",
    "1. **Reading and cleaning the Zillow rental data**:\n",
    "   - Loading the Zillow data from a CSV file\n",
    "   - Melting the data so that each row represents a unique date-region pair\n",
    "   - Filtering the data to include only New York City and the relevant date range (February 2022 to January 2024)\n",
    "   - Keeping the required columns (zipcode, city, date, rent price) and renaming them\n",
    "   - Converting the \"zipcode\" column to a string type and the \"date\" column to a datetime type\n",
    "\n",
    "2. **Reading and cleaning the zipcode data**\n",
    "3. **Downloading, cleaning, and filtering the 311 complaints data and tree data**\n",
    "4. **Filtering all the datasets to include only the cleaned zipcode data**\n",
    "\n",
    "5. **Performing data quality checks**:\n",
    "   - Checking for null values in each dataset\n",
    "   - Checking for duplicate entries in each dataset\n",
    "   - Cross-referencing the zipcodes across the datasets to ensure consistency\n",
    "6. **Show information and first 5 entries of each dataset**.\n",
    "\n",
    "Overall, the purpose of this part of the code is to extract, clean, and integrate the necessary information from the original data sources, preparing the data for further analysis. It involves key steps such as data loading, data cleaning, data filtering, and data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90e596",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zipcode_data()` function \n",
    "reads in a shapefile containing zipcode data, cleans and preprocesses the data, and returns a GeoDataFrame with unique zipcodes and their corresponding geometries. \n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading the shapefile using Geopandas, a library for working with geospatial data.\n",
    "2. Selecting the relevant columns (zipcode and geometry) and renaming the 'ZIPCODE' column to 'zipcode' for better readability.\n",
    "3. Converting the coordinate reference system (CRS) of the GeoDataFrame to EPSG:4326 (WGS84) for consistency.\n",
    "4. Removing any duplicate zipcode entries, keeping only the first occurrence of each unique zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zipcode_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean zipcode data from a shapefile.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing unique zipcodes and their geometries.\n",
    "    \"\"\"\n",
    "    # Read the shapefile using Geopandas\n",
    "    zipcode_data = gpd.read_file(ZIPCODE_DATA_FILE)\n",
    "    \n",
    "    # Select relevant columns and rename them\n",
    "    zipcode_cleaned = zipcode_data[['ZIPCODE', 'geometry']].rename(columns={'ZIPCODE': 'zipcode'})\n",
    "    \n",
    "    # Convert CRS to EPSG:4326 for consistency\n",
    "    zipcode_cleaned = zipcode_cleaned.to_crs(epsg=4326)\n",
    "    \n",
    "    # Remove duplicate zipcodes, keeping the first occurrence\n",
    "    zipcode_cleaned = zipcode_cleaned.drop_duplicates(subset=['zipcode'], keep='first')\n",
    "    \n",
    "    return zipcode_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd245d",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_311_data()` function \n",
    "fetches 311 complaint data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned 311 complaints.\n",
    "\n",
    "The key steps are:\n",
    "1. Defining the API parameters to fetch 311 complaint data within a specific date range and with valid latitude/longitude coordinates.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and setting the appropriate coordinate reference system (EPSG:4326).\n",
    "4. Selecting and renaming the relevant columns, and dropping any rows with missing zipcodes.\n",
    "5. Converting the 'created_date' column from a string to a date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean 311 complaint data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing 311 complaints with relevant fields and valid zipcodes.\n",
    "    \"\"\"\n",
    "    # API parameters for fetching data\n",
    "    complaints_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$where': 'created_date >= \"2022-02-01T00:00:00.000\" AND created_date <= \"2024-02-29T00:00:00.000\" AND latitude IS NOT NULL',\n",
    "        '$limit': 1000000\n",
    "    }\n",
    "    \n",
    "    # Requesting data from the API\n",
    "    complaints_response = requests.get(COMPLAINTS_URL, params=complaints_params)\n",
    "    if complaints_response.status_code != 200:\n",
    "        raise Exception(\"Failed to download data\")\n",
    "\n",
    "    # Create a GeoDataFrame from the response\n",
    "    complaints_data = gpd.GeoDataFrame.from_features(complaints_response.json()['features']).set_crs(epsg=4326)\n",
    "    \n",
    "    # Select and rename columns, and drop rows without zipcodes\n",
    "    complaints_cleaned = complaints_data[['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'geometry']].rename(columns={\n",
    "        'unique_key': 'unique_id', \n",
    "        'incident_zip': 'zipcode'\n",
    "    }).dropna(subset=['zipcode'])\n",
    "    \n",
    "    # Convert 'created_date' from string to date\n",
    "    complaints_cleaned['created_date'] = pd.to_datetime(complaints_cleaned['created_date']).dt.date\n",
    "    \n",
    "    return complaints_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61566a",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_tree_data()` function \n",
    "fetches tree data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned tree data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Defining the API parameters to fetch tree data, with a limit of 10 million records.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and dropping any rows where the latitude or longitude is missing.\n",
    "4. Converting the latitude and longitude columns to float and creating geometry points from them.\n",
    "5. Selecting and renaming the relevant columns, and dropping any rows where critical information (zipcode, health, or species) is missing.\n",
    "6. Setting the coordinate reference system (EPSG:4326) for the GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean tree data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing tree data with geometries created from latitude and longitude.\n",
    "    \"\"\"\n",
    "    trees_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$limit': 10000000\n",
    "    }\n",
    "    \n",
    "    trees_response = requests.get(TREES_URL, params=trees_params)\n",
    "    if trees_response.status_code != 200:\n",
    "        print(f\"Failed to download tree data. Status code: {trees_response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Create a GeoDataFrame from the JSON response\n",
    "    trees_data = gpd.GeoDataFrame.from_features(trees_response.json())\n",
    "\n",
    "    # Drop rows where latitude or longitude is NaN, and convert them to float\n",
    "    trees_data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    trees_data['latitude'] = trees_data['latitude'].astype(float)\n",
    "    trees_data['longitude'] = trees_data['longitude'].astype(float)\n",
    "\n",
    "    # Create geometry points from latitude and longitude\n",
    "    trees_data['geometry'] = trees_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "    # Select and rename columns, drop rows where any critical information is missing\n",
    "    trees_cleaned = trees_data[['tree_id', 'spc_common', 'health', 'status', 'zipcode', 'geometry']].rename(\n",
    "        columns={'spc_common': 'species'}\n",
    "    )\n",
    "    trees_cleaned['zipcode'] = trees_cleaned['zipcode'].astype(str)\n",
    "    trees_cleaned.crs = 'EPSG:4326'\n",
    "    trees_cleaned.dropna(subset=['zipcode', 'health', 'species'], inplace=True)\n",
    "\n",
    "    return trees_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a0dd9",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zillow_data()` function \n",
    "reads Zillow rental data for New York City from a CSV file, cleans and preprocesses the data, and returns a cleaned DataFrame.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Loading the Zillow data from a CSV file into a DataFrame.\n",
    "2. Melting the data so that each row represents a unique date-region pair, making it easier to work with.\n",
    "3. Filtering the data to include only New York City and selecting the relevant date range (February 2022 to January 2024).\n",
    "4. Keeping only the required columns (zipcode, city, data_date, rent_price) and renaming them for better readability.\n",
    "5. Converting the 'zipcode' column to a string and the 'data_date' column to a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a126471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zillow_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean Zillow rental data for New York City.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A cleaned DataFrame containing Zillow rental data with selected columns and filtered dates.\n",
    "    \"\"\"\n",
    "    # Load Zillow data from a CSV file\n",
    "    zillow_data = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "    \n",
    "    # Melt the data so that every row is a unique date-region pair\n",
    "    id_vars = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName']\n",
    "    melted_data = pd.melt(zillow_data, id_vars=id_vars, var_name='data_date', value_name='rent_price')\n",
    "    \n",
    "    # Filter for New York City data and select relevant dates\n",
    "    zillow_cleaned = melted_data[\n",
    "        (melted_data['City'] == 'New York') &\n",
    "        (melted_data['data_date'] >= '2022-02-01') &\n",
    "        (melted_data['data_date'] <= '2024-01-31')\n",
    "    ]\n",
    "    \n",
    "    # Keep only the required columns and rename them\n",
    "    zillow_cleaned = zillow_cleaned[['RegionName', 'City', 'data_date', 'rent_price']].rename(\n",
    "        columns={'RegionName': 'zipcode', 'City': 'city', 'data_date': 'data_date', 'rent_price': 'rent_price'}\n",
    "    ).dropna()\n",
    "    \n",
    "    # Convert data types\n",
    "    zillow_cleaned['zipcode'] = zillow_cleaned['zipcode'].astype(str)\n",
    "    zillow_cleaned['data_date'] = pd.to_datetime(zillow_cleaned['data_date'])\n",
    "\n",
    "    return zillow_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d12aa",
   "metadata": {},
   "source": [
    "#### The `load_and_clean_all_data()` function \n",
    "is responsible for loading and cleaning multiple datasets, including zip codes, Zillow rental data, 311 complaints, and tree data. It then filters all the datasets to only include entries corresponding to the cleaned zip code data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading and cleaning the zip code data using the `read_and_clean_zipcode_data()` function.\n",
    "2. Reading, cleaning, and filtering the Zillow rental data using the `read_and_clean_zillow_data()` function, ensuring that only the data for the cleaned zip codes is retained.\n",
    "3. Downloading, cleaning, and filtering the 311 complaints data using the `download_and_clean_311_data()` function, again ensuring that only the data for the cleaned zip codes is kept.\n",
    "4. Downloading, cleaning, and filtering the tree data using the `download_and_clean_tree_data()` function, keeping only the data for the cleaned zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_all_data() -> Tuple[gpd.GeoDataFrame, pd.DataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean all relevant datasets including zip codes, rental data, 311 complaints, and tree data,\n",
    "    filtering them to only include entries corresponding to the cleaned zip code data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[GeoDataFrame, DataFrame, GeoDataFrame, GeoDataFrame]: A tuple containing the cleaned data for\n",
    "        zip codes, Zillow rental, 311 complaints, and tree data respectively.\n",
    "    \"\"\"\n",
    "    # Read and clean zipcode data\n",
    "    zipcode_data = read_and_clean_zipcode_data()\n",
    "    \n",
    "    # Read, clean and filter Zillow rental data\n",
    "    zillow_data = read_and_clean_zillow_data()\n",
    "    zillow_data = zillow_data[zillow_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter 311 complaints data\n",
    "    complaints_data = download_and_clean_311_data()\n",
    "    if complaints_data is not None:\n",
    "        complaints_data = complaints_data[complaints_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter tree data\n",
    "    tree_data = download_and_clean_tree_data()\n",
    "    if tree_data is not None:\n",
    "        tree_data = tree_data[tree_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "\n",
    "    return zipcode_data, zillow_data, complaints_data, tree_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a1a81",
   "metadata": {},
   "source": [
    "#### Load and clean all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean all data sets\n",
    "zipcode_data, zillow_data, complaints_data, tree_data = load_and_clean_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22600da1",
   "metadata": {},
   "source": [
    "#### The `check_data_quality()` function \n",
    "performs a series of data quality checks on the loaded datasets, including:\n",
    "\n",
    "1. Checking for null values in each dataset (zipcode_data, zillow_data, complaints_data, and tree_data).\n",
    "2. Checking for duplicate entries in each dataset, such as duplicate zipcodes, tree IDs, and unique IDs.\n",
    "3. Cross-referencing the zipcodes across the datasets to ensure that all zipcodes in the Zillow rental, 311 complaints, and tree data are present in the cleaned zipcode_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a52ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality():\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for null values in each dataset\n",
    "    print(\"Null values in zipcode_data:\", zipcode_data.isnull().sum())\n",
    "    print(\"Null values in zillow_data:\", zillow_data.isnull().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Null values in complaints_data:\", complaints_data.isnull().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Null values in tree_data:\", tree_data.isnull().sum())\n",
    "\n",
    "    # Check for duplicate entries\n",
    "    print(\"Duplicate zipcodes in zipcode_data:\", zipcode_data['zipcode'].duplicated().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Duplicate tree_ids in tree_data:\", tree_data['tree_id'].duplicated().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Duplicate unique_ids in complaints_data:\", complaints_data['unique_id'].duplicated().sum())\n",
    "    print(\"Duplicate zipcodes in zillow_data:\", zillow_data['zipcode'].duplicated().sum())\n",
    "\n",
    "    # Cross-reference zipcodes across datasets\n",
    "    if zillow_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all zillow_data zipcodes in zipcode_data?\", all(zillow_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if complaints_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all complaints_data zipcodes in zipcode_data?\", all(complaints_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if tree_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all tree_data zipcodes in zipcode_data?\", all(tree_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "\n",
    "# Run the data quality checks\n",
    "check_data_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d66b2",
   "metadata": {},
   "source": [
    "#### Show basic info about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038dfe3",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaf6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a02f4",
   "metadata": {},
   "source": [
    "#### Show basic info about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76679837",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa76d09",
   "metadata": {},
   "source": [
    "#### Show basic info about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd949aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8dddb2",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f51f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b134733",
   "metadata": {},
   "source": [
    "#### Show basic info about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d492bd1",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893335ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
