{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7512a413",
   "metadata": {},
   "source": [
    "# NYC Apartment Search - Group 1\n",
    "\n",
    "### Purpose of the Project:\n",
    "The project uses data-driven approaches to analyze and visualize New York City apartment data, 311 complaints, and urban forestry data to help understand urban living dynamics. This analysis is intended to aid in making informed decisions about apartment rentals based on environmental and urban living conditions.\n",
    "\n",
    "### Sections and Key Functions:\n",
    "1. **Setup**\n",
    "   - Initializes the environment with necessary libraries and settings.\n",
    "\n",
    "2. **Part 1: Data Preprocessing**\n",
    "   - Functions to load and clean data from various sources (ZIP codes, 311 complaints, tree census, Zillow rent data).\n",
    "   - Quality checks and basic data explorations are conducted.\n",
    "\n",
    "3. **Part 2: Storing Data**\n",
    "   - Database setup functions to create tables and indices.\n",
    "   - Functions to convert geometries for database insertion and to insert cleaned data into a PostgreSQL database.\n",
    "   - Data retrieval functions to fetch and display samples from each database table.\n",
    "\n",
    "4. **Part 3: Understanding the Data**\n",
    "   - Functions to execute SQL queries and to extract meaningful insights from the database.\n",
    "   - Various SQL queries analyze the relationship between apartment prices, complaints, and tree census data.\n",
    "\n",
    "5. **Part 4: Visualizing the Data**\n",
    "   - Multiple visualizations to represent data insights graphically, including trends over time and spatial distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b027f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.base import Engine\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2 import Geometry, WKTElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d09e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"nyc_zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "QUERY_DIR = pathlib.Path(\"queries\")  # Directory for saving DB queries\n",
    "\n",
    "# API configuration\n",
    "APP_TOKEN = \"J9t5fS2TcfDISWng9WsnCdvCP\"\n",
    "COMPLAINTS_URL = 'https://data.cityofnewyork.us/resource/erm2-nwe9.geojson'\n",
    "TREES_URL = 'https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson'\n",
    "\n",
    "# Database configuration\n",
    "DB_NAME = \"nyc_data\"\n",
    "DB_USER = \"williamsjs\"\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory: pathlib.Path):\n",
    "    \"\"\"Ensure that a directory exists; if not, create it.\"\"\"\n",
    "    try:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {directory}: {e}\")\n",
    "\n",
    "# Make sure the directories exist\n",
    "ensure_directory_exists(DATA_DIR)\n",
    "ensure_directory_exists(QUERY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119d826",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "The first part of the data cleaning process involves the following steps:\n",
    "\n",
    "1. **Reading and cleaning the Zillow rental data**:\n",
    "   - Loading the Zillow data from a CSV file\n",
    "   - Melting the data so that each row represents a unique date-region pair\n",
    "   - Filtering the data to include only New York City and the relevant date range (February 2022 to January 2024)\n",
    "   - Keeping the required columns (zipcode, city, date, rent price) and renaming them\n",
    "   - Converting the \"zipcode\" column to a string type and the \"date\" column to a datetime type\n",
    "\n",
    "2. **Reading and cleaning the zipcode data**\n",
    "3. **Downloading, cleaning, and filtering the 311 complaints data and tree data**\n",
    "4. **Filtering all the datasets to include only the cleaned zipcode data**\n",
    "\n",
    "5. **Performing data quality checks**:\n",
    "   - Checking for null values in each dataset\n",
    "   - Checking for duplicate entries in each dataset\n",
    "   - Cross-referencing the zipcodes across the datasets to ensure consistency\n",
    "6. **Show information and first 5 entries of each dataset**.\n",
    "\n",
    "Overall, the purpose of this part of the code is to extract, clean, and integrate the necessary information from the original data sources, preparing the data for further analysis. It involves key steps such as data loading, data cleaning, data filtering, and data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90e596",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zipcode_data()` function \n",
    "reads in a shapefile containing zipcode data, cleans and preprocesses the data, and returns a GeoDataFrame with unique zipcodes and their corresponding geometries. \n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading the shapefile using Geopandas, a library for working with geospatial data.\n",
    "2. Selecting the relevant columns (zipcode and geometry) and renaming the 'ZIPCODE' column to 'zipcode' for better readability.\n",
    "3. Converting the coordinate reference system (CRS) of the GeoDataFrame to EPSG:4326 (WGS84) for consistency.\n",
    "4. Removing any duplicate zipcode entries, keeping only the first occurrence of each unique zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zipcode_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean zipcode data from a shapefile.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing unique zipcodes and their geometries.\n",
    "    \"\"\"\n",
    "    # Read the shapefile using Geopandas\n",
    "    zipcode_data = gpd.read_file(ZIPCODE_DATA_FILE)\n",
    "    \n",
    "    # Select relevant columns and rename them\n",
    "    zipcode_cleaned = zipcode_data[['ZIPCODE', 'geometry']].rename(columns={'ZIPCODE': 'zipcode'})\n",
    "    \n",
    "    # Convert CRS to EPSG:4326 for consistency\n",
    "    zipcode_cleaned = zipcode_cleaned.to_crs(epsg=4326)\n",
    "    \n",
    "    # Remove duplicate zipcodes, keeping the first occurrence\n",
    "    zipcode_cleaned = zipcode_cleaned.drop_duplicates(subset=['zipcode'], keep='first')\n",
    "    \n",
    "    return zipcode_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd245d",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_311_data()` function \n",
    "fetches 311 complaint data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned 311 complaints.\n",
    "\n",
    "The key steps are:\n",
    "1. Defining the API parameters to fetch 311 complaint data within a specific date range and with valid latitude/longitude coordinates.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and setting the appropriate coordinate reference system (EPSG:4326).\n",
    "4. Selecting and renaming the relevant columns, and dropping any rows with missing zipcodes.\n",
    "5. Converting the 'created_date' column from a string to a date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean 311 complaint data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing 311 complaints with relevant fields and valid zipcodes.\n",
    "    \"\"\"\n",
    "    # API parameters for fetching data\n",
    "    complaints_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$where': 'created_date >= \"2022-02-01T00:00:00.000\" AND created_date <= \"2024-02-29T00:00:00.000\" AND latitude IS NOT NULL',\n",
    "        '$limit': 1000000\n",
    "    }\n",
    "    \n",
    "    # Requesting data from the API\n",
    "    complaints_response = requests.get(COMPLAINTS_URL, params=complaints_params)\n",
    "    if complaints_response.status_code != 200:\n",
    "        raise Exception(\"Failed to download data\")\n",
    "\n",
    "    # Create a GeoDataFrame from the response\n",
    "    complaints_data = gpd.GeoDataFrame.from_features(complaints_response.json()['features']).set_crs(epsg=4326)\n",
    "    \n",
    "    # Select and rename columns, and drop rows without zipcodes\n",
    "    complaints_cleaned = complaints_data[['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'geometry']].rename(columns={\n",
    "        'unique_key': 'unique_id', \n",
    "        'incident_zip': 'zipcode'\n",
    "    }).dropna(subset=['zipcode'])\n",
    "    \n",
    "    # Convert 'created_date' from string to date\n",
    "    complaints_cleaned['created_date'] = pd.to_datetime(complaints_cleaned['created_date']).dt.date\n",
    "    \n",
    "    return complaints_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61566a",
   "metadata": {},
   "source": [
    "#### The `download_and_clean_tree_data()` function \n",
    "fetches tree data from the NYC Open Data API, cleans and preprocesses the data, and returns a GeoDataFrame containing the cleaned tree data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Defining the API parameters to fetch tree data, with a limit of 10 million records.\n",
    "2. Sending a request to the API and handling any errors that may occur during the download.\n",
    "3. Creating a GeoDataFrame from the API response and dropping any rows where the latitude or longitude is missing.\n",
    "4. Converting the latitude and longitude columns to float and creating geometry points from them.\n",
    "5. Selecting and renaming the relevant columns, and dropping any rows where critical information (zipcode, health, or species) is missing.\n",
    "6. Setting the coordinate reference system (EPSG:4326) for the GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Download and clean tree data from the NYC Open Data API.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A cleaned GeoDataFrame containing tree data with geometries created from latitude and longitude.\n",
    "    \"\"\"\n",
    "    trees_params = {\n",
    "        '$$app_token': APP_TOKEN,\n",
    "        '$limit': 10000000\n",
    "    }\n",
    "    \n",
    "    trees_response = requests.get(TREES_URL, params=trees_params)\n",
    "    if trees_response.status_code != 200:\n",
    "        print(f\"Failed to download tree data. Status code: {trees_response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Create a GeoDataFrame from the JSON response\n",
    "    trees_data = gpd.GeoDataFrame.from_features(trees_response.json())\n",
    "\n",
    "    # Drop rows where latitude or longitude is NaN, and convert them to float\n",
    "    trees_data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    trees_data['latitude'] = trees_data['latitude'].astype(float)\n",
    "    trees_data['longitude'] = trees_data['longitude'].astype(float)\n",
    "\n",
    "    # Create geometry points from latitude and longitude\n",
    "    trees_data['geometry'] = trees_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "    # Select and rename columns, drop rows where any critical information is missing\n",
    "    trees_cleaned = trees_data[['tree_id', 'spc_common', 'health', 'status', 'zipcode', 'geometry']].rename(\n",
    "        columns={'spc_common': 'species'}\n",
    "    )\n",
    "    trees_cleaned['zipcode'] = trees_cleaned['zipcode'].astype(str)\n",
    "    trees_cleaned.crs = 'EPSG:4326'\n",
    "    trees_cleaned.dropna(subset=['zipcode', 'health', 'species'], inplace=True)\n",
    "\n",
    "    return trees_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a0dd9",
   "metadata": {},
   "source": [
    "#### The `read_and_clean_zillow_data()` function \n",
    "reads Zillow rental data for New York City from a CSV file, cleans and preprocesses the data, and returns a cleaned DataFrame.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Loading the Zillow data from a CSV file into a DataFrame.\n",
    "2. Melting the data so that each row represents a unique date-region pair, making it easier to work with.\n",
    "3. Filtering the data to include only New York City and selecting the relevant date range (February 2022 to January 2024).\n",
    "4. Keeping only the required columns (zipcode, city, data_date, rent_price) and renaming them for better readability.\n",
    "5. Converting the 'zipcode' column to a string and the 'data_date' column to a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a126471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_zillow_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and clean Zillow rental data for New York City.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A cleaned DataFrame containing Zillow rental data with selected columns and filtered dates.\n",
    "    \"\"\"\n",
    "    # Load Zillow data from a CSV file\n",
    "    zillow_data = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "    \n",
    "    # Melt the data so that every row is a unique date-region pair\n",
    "    id_vars = ['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName']\n",
    "    melted_data = pd.melt(zillow_data, id_vars=id_vars, var_name='data_date', value_name='rent_price')\n",
    "    \n",
    "    # Filter for New York City data and select relevant dates\n",
    "    zillow_cleaned = melted_data[\n",
    "        (melted_data['City'] == 'New York') &\n",
    "        (melted_data['data_date'] >= '2022-02-01') &\n",
    "        (melted_data['data_date'] <= '2024-01-31')\n",
    "    ]\n",
    "    \n",
    "    # Keep only the required columns and rename them\n",
    "    zillow_cleaned = zillow_cleaned[['RegionName', 'City', 'data_date', 'rent_price']].rename(\n",
    "        columns={'RegionName': 'zipcode', 'City': 'city', 'data_date': 'data_date', 'rent_price': 'rent_price'}\n",
    "    ).dropna()\n",
    "    \n",
    "    # Convert data types\n",
    "    zillow_cleaned['zipcode'] = zillow_cleaned['zipcode'].astype(str)\n",
    "    zillow_cleaned['data_date'] = pd.to_datetime(zillow_cleaned['data_date'])\n",
    "\n",
    "    return zillow_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d12aa",
   "metadata": {},
   "source": [
    "#### The `load_and_clean_all_data()` function \n",
    "is responsible for loading and cleaning multiple datasets, including zip codes, Zillow rental data, 311 complaints, and tree data. It then filters all the datasets to only include entries corresponding to the cleaned zip code data.\n",
    "\n",
    "The key steps are:\n",
    "\n",
    "1. Reading and cleaning the zip code data using the `read_and_clean_zipcode_data()` function.\n",
    "2. Reading, cleaning, and filtering the Zillow rental data using the `read_and_clean_zillow_data()` function, ensuring that only the data for the cleaned zip codes is retained.\n",
    "3. Downloading, cleaning, and filtering the 311 complaints data using the `download_and_clean_311_data()` function, again ensuring that only the data for the cleaned zip codes is kept.\n",
    "4. Downloading, cleaning, and filtering the tree data using the `download_and_clean_tree_data()` function, keeping only the data for the cleaned zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_all_data() -> Tuple[gpd.GeoDataFrame, pd.DataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load and clean all relevant datasets including zip codes, rental data, 311 complaints, and tree data,\n",
    "    filtering them to only include entries corresponding to the cleaned zip code data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[GeoDataFrame, DataFrame, GeoDataFrame, GeoDataFrame]: A tuple containing the cleaned data for\n",
    "        zip codes, Zillow rental, 311 complaints, and tree data respectively.\n",
    "    \"\"\"\n",
    "    # Read and clean zipcode data\n",
    "    zipcode_data = read_and_clean_zipcode_data()\n",
    "    \n",
    "    # Read, clean and filter Zillow rental data\n",
    "    zillow_data = read_and_clean_zillow_data()\n",
    "    zillow_data = zillow_data[zillow_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter 311 complaints data\n",
    "    complaints_data = download_and_clean_311_data()\n",
    "    if complaints_data is not None:\n",
    "        complaints_data = complaints_data[complaints_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "    \n",
    "    # Download, clean and filter tree data\n",
    "    tree_data = download_and_clean_tree_data()\n",
    "    if tree_data is not None:\n",
    "        tree_data = tree_data[tree_data['zipcode'].isin(zipcode_data['zipcode'])]\n",
    "\n",
    "    return zipcode_data, zillow_data, complaints_data, tree_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a1a81",
   "metadata": {},
   "source": [
    "#### Load and clean all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean all data sets\n",
    "zipcode_data, zillow_data, complaints_data, tree_data = load_and_clean_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22600da1",
   "metadata": {},
   "source": [
    "#### The `check_data_quality()` function \n",
    "performs a series of data quality checks on the loaded datasets, including:\n",
    "\n",
    "1. Checking for null values in each dataset (zipcode_data, zillow_data, complaints_data, and tree_data).\n",
    "2. Checking for duplicate entries in each dataset, such as duplicate zipcodes, tree IDs, and unique IDs.\n",
    "3. Cross-referencing the zipcodes across the datasets to ensure that all zipcodes in the Zillow rental, 311 complaints, and tree data are present in the cleaned zipcode_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a52ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality():\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for null values in each dataset\n",
    "    print(\"Null values in zipcode_data:\", zipcode_data.isnull().sum())\n",
    "    print(\"Null values in zillow_data:\", zillow_data.isnull().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Null values in complaints_data:\", complaints_data.isnull().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Null values in tree_data:\", tree_data.isnull().sum())\n",
    "\n",
    "    # Check for duplicate entries\n",
    "    print(\"Duplicate zipcodes in zipcode_data:\", zipcode_data['zipcode'].duplicated().sum())\n",
    "    if tree_data is not None:\n",
    "        print(\"Duplicate tree_ids in tree_data:\", tree_data['tree_id'].duplicated().sum())\n",
    "    if complaints_data is not None:\n",
    "        print(\"Duplicate unique_ids in complaints_data:\", complaints_data['unique_id'].duplicated().sum())\n",
    "    print(\"Duplicate zipcodes in zillow_data:\", zillow_data['zipcode'].duplicated().sum())\n",
    "\n",
    "    # Cross-reference zipcodes across datasets\n",
    "    if zillow_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all zillow_data zipcodes in zipcode_data?\", all(zillow_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if complaints_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all complaints_data zipcodes in zipcode_data?\", all(complaints_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "    if tree_data is not None and zipcode_data is not None:\n",
    "        print(\"Are all tree_data zipcodes in zipcode_data?\", all(tree_data['zipcode'].isin(zipcode_data['zipcode'].unique())))\n",
    "\n",
    "# Run the data quality checks\n",
    "check_data_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d66b2",
   "metadata": {},
   "source": [
    "#### Show basic info about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038dfe3",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zipcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaf6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a02f4",
   "metadata": {},
   "source": [
    "#### Show basic info about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76679837",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about complaints_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa76d09",
   "metadata": {},
   "source": [
    "#### Show basic info about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd949aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8dddb2",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f51f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b134733",
   "metadata": {},
   "source": [
    "#### Show basic info about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d492bd1",
   "metadata": {},
   "source": [
    "#### Show first 5 entries about zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893335ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dd289",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data\n",
    "\n",
    "The second part of the data storage process involves the following steps:\n",
    "\n",
    "1. **Defining and Applying the Database Schema `(define_and_apply_schema())`**:\n",
    "   - Created four tables: zipcode_data, complaints_data, tree_data, and zillow_data\n",
    "   - Defined the table structure, including primary keys, foreign keys, and other constraints\n",
    "   - Created spatial indexes on the geometry fields to improve query performance\n",
    "   - Wrote the defined schema to an SQL file and executed it to apply the schema to the database\n",
    "\n",
    "2. **Converting Geometries for Database Insertion `(convert_geom())`**:\n",
    "   - Converted the geometry data in the GeoDataFrame to Well-Known Text (WKT) format for storage in the database\n",
    "\n",
    "3. **Inserting Data into the Database `(insert_data())`**:\n",
    "   - Used the pandas `to_sql()` method to insert the cleaned datasets into the corresponding database tables\n",
    "   - Specified the data types, ensuring the geometry data is stored correctly\n",
    "   - Handled any exceptions that may occur during the data insertion process\n",
    "\n",
    "4. **Fetching Data from the Database `(fetch_data())`**:\n",
    "   - Defined a generic function to use pandas' `read_sql_query()` to retrieve data from the database based on a provided SQL query\n",
    "   - Provided an example usage, querying the row counts for each of the tables and returning the results\n",
    "\n",
    "In summary, this part of the code completed the process of storing the cleaned data in a PostgreSQL database, and provided related read and write interfaces, laying the foundation for further data analysis and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f993af",
   "metadata": {},
   "source": [
    "#### The `create_database()` function \n",
    "\n",
    "performs the following key steps:\n",
    "\n",
    "1. Creates a new PostgreSQL database named 'nyc_data' using the `createdb` command.\n",
    "2. Adds the PostGIS extension to the newly created database using the `CREATE EXTENSION postgis;` SQL command.\n",
    "3. If the database creation and PostGIS extension addition are successful, it prints a success message. If there is an error, it prints the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \"\"\"\n",
    "    Creates a new PostgreSQL database named 'nyc_data' and adds PostGIS extension.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"createdb\", DB_NAME], check=True)\n",
    "        subprocess.run([\"psql\", \"--dbname\", DB_NAME, \"-c\", \"CREATE EXTENSION postgis;\"], check=True)\n",
    "        print(\"Database and PostGIS extension created successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to create database or PostGIS extension: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6f6e4",
   "metadata": {},
   "source": [
    "#### The `define_and_apply_schema()` function \n",
    "\n",
    "performs the following key steps:\n",
    "\n",
    "1. It defines the schema for the database, including the creation of four tables: `zipcode_data`, `complaints_data`, `tree_data`, and `zillow_data`. Each table has a specific set of columns and constraints, such as primary keys and foreign key relationships.\n",
    "2. The function also creates spatial indices on the geometry columns of the tables, which will improve the performance of spatial queries.\n",
    "3. The defined schema is written to a SQL file, and then executed using the `psql` command to apply the schema to the 'nyc_data' database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290161cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_apply_schema():\n",
    "    \"\"\"\n",
    "    Defines the schema for the database and applies it using SQL file execution.\n",
    "    \"\"\"\n",
    "    schema_sql = \"\"\"\n",
    "        -- Drop existing tables and indices if they exist\n",
    "        DROP INDEX IF EXISTS idx_zipcode_geom CASCADE;\n",
    "        DROP INDEX IF EXISTS idx_complaints_geom CASCADE;\n",
    "        DROP INDEX IF EXISTS idx_tree_geom CASCADE;\n",
    "        DROP TABLE IF EXISTS complaints_data CASCADE;\n",
    "        DROP TABLE IF EXISTS tree_data CASCADE;\n",
    "        DROP TABLE IF EXISTS zillow_data CASCADE;\n",
    "        DROP TABLE IF EXISTS zipcode_data CASCADE;\n",
    "\n",
    "        -- Create tables\n",
    "        CREATE TABLE zipcode_data (\n",
    "            zipcode TEXT PRIMARY KEY,\n",
    "            geom GEOMETRY\n",
    "        );\n",
    "\n",
    "        CREATE TABLE complaints_data (\n",
    "            unique_id BIGINT PRIMARY KEY,\n",
    "            created_date DATE,\n",
    "            complaint_type TEXT,\n",
    "            zipcode TEXT,\n",
    "            geom GEOMETRY,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE tree_data (\n",
    "            tree_id BIGINT PRIMARY KEY,\n",
    "            species TEXT,\n",
    "            health TEXT,\n",
    "            status TEXT,\n",
    "            zipcode TEXT,\n",
    "            geom GEOMETRY,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE zillow_data (\n",
    "            zipcode TEXT,\n",
    "            city TEXT,\n",
    "            data_date DATE,\n",
    "            rent_price NUMERIC,\n",
    "            FOREIGN KEY (zipcode) REFERENCES zipcode_data(zipcode),\n",
    "            PRIMARY KEY (zipcode, data_date)\n",
    "        );\n",
    "\n",
    "        -- Create spatial indices\n",
    "        CREATE INDEX idx_zipcode_geom ON zipcode_data USING gist(geom);\n",
    "        CREATE INDEX idx_complaints_geom ON complaints_data USING gist(geom);\n",
    "        CREATE INDEX idx_tree_geom ON tree_data USING gist(geom);\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the SQL schema to a file\n",
    "    with open(\"schema.sql\", \"w\") as file:\n",
    "        file.write(schema_sql)\n",
    "    \n",
    "    # Execute the SQL schema file\n",
    "    try:\n",
    "        subprocess.run([\"psql\", \"-d\", DB_NAME, \"-f\", \"schema.sql\"], check=True)\n",
    "        print(\"Database schema applied successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to apply database schema: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    define_and_apply_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e44b85",
   "metadata": {},
   "source": [
    "#### The `convert_geom()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a GeoDataFrame as input, which contains a 'geometry' column with spatial data.\n",
    "2. The function checks if the 'geometry' column exists in the input DataFrame.\n",
    "3. If the 'geometry' column is present, it applies the `WKTElement()` function to each geometry object in the column, converting it to a Well-Known Text (WKT) representation.\n",
    "4. The function then renames the 'geometry' column to 'geom' and drops the original 'geometry' column, returning a new DataFrame with the converted geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806836f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_geom(data: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert geometries to WKTElement for database insertion.\n",
    "\n",
    "    Args:\n",
    "        data (GeoDataFrame): The geopandas DataFrame containing the 'geometry' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame with converted geometry to WKTElement and renamed to 'geom'.\n",
    "    \"\"\"\n",
    "    if 'geometry' in data.columns:\n",
    "        data['geom'] = data['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))\n",
    "        return data.drop(columns=['geometry'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ab9c2",
   "metadata": {},
   "source": [
    "#### The `insert_data()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a pandas DataFrame, the name of the target database table, and an optional dictionary of SQLAlchemy data types as input.\n",
    "2. The function uses the `to_sql()` method of the DataFrame to insert the data into the specified table in the database.\n",
    "3. The `if_exists='append'` parameter ensures that the data is appended to the table, rather than overwriting any existing data.\n",
    "4. The `index=False` parameter ensures that the DataFrame index is not included in the insert.\n",
    "5. The `dtype` parameter is used to specify the SQL column types for the data, which is particularly important for the 'geom' column containing the spatial data.\n",
    "6. If the data insertion is successful, it prints a success message. If there is an error, it prints the error message.\n",
    "7. The code calls the `insert_data()` function for each of the cleaned datasets (`zipcode_data`, `complaints_data`, `tree_data`, and `zillow_data`), using the `convert_geom()` function to convert the geometries to the appropriate format before insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baddfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(df: pd.DataFrame, table_name: str, dtype: dict = None) -> None:\n",
    "    \"\"\"\n",
    "    Inserts data into a database table.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to insert into the database.\n",
    "        table_name (str): The name of the target database table.\n",
    "        dtype (dict): A dictionary of SQLAlchemy types to specify SQL column types.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the data insertion fails, it raises an exception with the error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False, dtype=dtype)\n",
    "        print(f\"{table_name} inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert {table_name}: {str(e)}\")\n",
    "\n",
    "# use the above functions to insert datasets to database\n",
    "insert_data(convert_geom(zipcode_data), 'zipcode_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(convert_geom(complaints_data), 'complaints_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(convert_geom(tree_data), 'tree_data', {'geom': Geometry('GEOMETRY', srid=4326)})\n",
    "insert_data(zillow_data, 'zillow_data')\n",
    "\n",
    "print(\"Database setup and data insertion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00916a",
   "metadata": {},
   "source": [
    "#### The `fetch_data()` function \n",
    "performs the following key steps:\n",
    "\n",
    "1. It takes a SQL query and a database engine connection as input.\n",
    "2. The function uses the `pd.read_sql_query()` method to execute the provided SQL query and fetch the resulting data as a pandas DataFrame.\n",
    "3. If there is an error during the data fetch, the function prints the error message and returns an empty DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cce3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(query: str, engine: Engine) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches data from the database based on the provided SQL query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to be executed.\n",
    "        engine (Engine): The database engine connection used to execute the query.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the results of the SQL query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql_query(query, con=engine)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "\n",
    "# Example usage of the fetch_data function\n",
    "query = \"\"\"\n",
    "    SELECT 'zipcode_data' AS table_name, COUNT(*) AS total_rows FROM zipcode_data UNION ALL\n",
    "    SELECT 'complaints_data' AS table_name, COUNT(*) FROM complaints_data UNION ALL\n",
    "    SELECT 'tree_data' AS table_name, COUNT(*) FROM tree_data UNION ALL\n",
    "    SELECT 'zillow_data' AS table_name, COUNT(*) FROM zillow_data;\n",
    "\"\"\"\n",
    "row_counts = fetch_data(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a6806",
   "metadata": {},
   "source": [
    "#### Print the row counts for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e5983",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'zipcode_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_sample = fetch_data(\"SELECT * FROM zipcode_data LIMIT 5;\", engine)\n",
    "zipcode_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadf6bd",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'complaints_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_sample = fetch_data(\"SELECT * FROM complaints_data LIMIT 5;\", engine)\n",
    "complaints_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88b1c7",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'tree_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sample = fetch_data(\"SELECT * FROM tree_data LIMIT 5;\", engine)\n",
    "tree_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88bafc",
   "metadata": {},
   "source": [
    "#### Fetch and print a sample from the 'zillow_data' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99657952",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_sample = fetch_data(\"SELECT * FROM zillow_data LIMIT 5;\", engine)\n",
    "zillow_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b272a93",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n",
    "\n",
    "1. `QUERY1`: This query calculates the average rent price for each zipcode in the `zillow_data` table as of the date '2024-01-31'.\n",
    "\n",
    "2. `QUERY2`: This query counts the number of trees for each zipcode in the `tree_data` table.\n",
    "\n",
    "3. `QUERY3`: This query counts the number of complaints for each zipcode in the `complaints_data` table where the `created_date` is between '2024-01-01' and '2024-01-31'.\n",
    "\n",
    "4. `QUERY4`: This query finds the correlation between rent, trees, and complaints by joining the results of the previous three queries. It focuses on the top and bottom 5 zipcodes by average rent as of '2024-01-31'.\n",
    "\n",
    "5. `QUERY5`: This query calculates the percentage change in rent prices for each zipcode in the `zillow_data` table between the dates '2023-01-31' and '2024-01-31'.\n",
    "\n",
    "6. `QUERY6`: This query calculates the percentage change in the number of trees for each zipcode in the `tree_data` table between the dates '2023-01-01' and '2024-01-01'.\n",
    "\n",
    "These queries provide insights into the relationships between rent, trees, and complaints in the given data, as well as the changes in rent prices and tree counts over a one-year period from 2023 to 2024. The results can be used to analyze urban development and quality of life factors within the specified timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61eaa69",
   "metadata": {},
   "source": [
    "#### The `read_query()` function \n",
    "performs the following tasks:\n",
    "\n",
    "1. It takes an SQL query (as a string) and a database engine connection as input parameters.\n",
    "2. It attempts to execute the given SQL query using the provided database engine connection and returns the results as a pandas DataFrame.\n",
    "3. If an exception occurs during the query execution, the function prints an error message and returns an empty pandas DataFrame instead of raising the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(query: str, engine: Engine) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes an SQL query and returns the results as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "        engine (Engine): The database engine connection to use for the query.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql(query, con=engine)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute query: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabeb03",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "1. It defines an SQL query, `QUERY1`, that retrieves the number of 311 complaints per zipcode in New York City for the period between March 2023 and February 2024. The query groups the complaints by zipcode and orders the results by the complaint count in descending order.\n",
    "\n",
    "2. It writes the `QUERY1` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY1` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `complaint_counts` variable.\n",
    "\n",
    "4. Finally, it returns the `complaint_counts` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22966492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Number of 311 complaints per zip code\n",
    "# Define the query for the number of 311 complaints per zip code\n",
    "QUERY1 = \"\"\"\n",
    "    SELECT zipcode, COUNT(*) as complaint_count\n",
    "    FROM complaints_data\n",
    "    WHERE created_date >= '2023-03-01' AND created_date <= '2024-02-29'\n",
    "    GROUP BY zipcode\n",
    "    ORDER BY complaint_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file = QUERY_DIR / \"number_of_complaints_per_zipcode.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file, \"w\") as file:\n",
    "    file.write(QUERY1)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "complaint_counts = read_query(QUERY1, engine)\n",
    "complaint_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd7ea7",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "1. It defines an SQL query, `QUERY2`, that retrieves the top 10 zipcodes by tree count. The query groups the tree data by zipcode, counts the number of trees for each zipcode, orders the results by the tree count in descending order, and limits the output to the top 10 rows.\n",
    "\n",
    "2. It writes the `QUERY2` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY2` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `top_tree_counts` variable.\n",
    "\n",
    "4. Finally, it returns the `top_tree_counts` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2: Top 10 zip codes by tree count\n",
    "# Define the query for the top 10 zip codes by tree count\n",
    "QUERY2 = \"\"\"\n",
    "    SELECT zipcode, COUNT(*) as tree_count\n",
    "    FROM tree_data\n",
    "    GROUP BY zipcode\n",
    "    ORDER BY tree_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file2 = QUERY_DIR / \"top_10_zipcodes_by_tree_count.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file2, \"w\") as file:\n",
    "    file.write(QUERY2)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "top_tree_counts = read_query(QUERY2, engine)\n",
    "top_tree_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc1f8c",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "1. It defines an SQL query, `QUERY3`, that retrieves the average rent in the top 10 zipcodes with the most trees. The query uses a common table expression (CTE) to first identify the top 10 zipcodes by tree count, and then joins this with the Zillow rental data to calculate the average rent for each of those top 10 zipcodes.\n",
    "\n",
    "2. It writes the `QUERY3` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY3` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `average_rent_in_greenest_areas` variable.\n",
    "\n",
    "4. Finally, it returns the `average_rent_in_greenest_areas` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 3: Average rent in zip codes with the most trees\n",
    "# Define the query for the average rent in zip codes with the most trees\n",
    "QUERY3 = \"\"\"\n",
    "    WITH TopTreeZipCodes AS (\n",
    "        SELECT zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data\n",
    "        GROUP BY zipcode\n",
    "        ORDER BY tree_count DESC\n",
    "        LIMIT 10\n",
    "    )\n",
    "    SELECT t.zipcode, TO_CHAR(AVG(z.rent_price), 'FM9,999,999.00') as average_rent\n",
    "    FROM TopTreeZipCodes t\n",
    "    JOIN zillow_data z ON t.zipcode = z.zipcode AND z.data_date = '2024-01-31'\n",
    "    GROUP BY t.zipcode, t.tree_count\n",
    "    ORDER BY t.tree_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file3 = QUERY_DIR / \"average_rent_in_greenest_areas.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file3, \"w\") as file:\n",
    "    file.write(QUERY3)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "average_rent_in_greenest_areas = read_query(QUERY3, engine)\n",
    "average_rent_in_greenest_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec1f28",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "1. It defines an SQL query, `QUERY4`, that retrieves the correlation between rent, trees, and complaints for the top and bottom 5 zipcodes by average rent. The query uses three common table expressions (CTEs) to:\n",
    "   - Calculate the average rent and rent ranking for each zipcode\n",
    "   - Count the number of trees for each zipcode\n",
    "   - Count the number of complaints for each zipcode in January 2024\n",
    "\n",
    "2. It joins the results of these three CTEs to create a final result set that includes the zipcode, average rent, tree count, and complaint count for the top and bottom 5 zipcodes by average rent.\n",
    "\n",
    "3. It writes the `QUERY4` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "4. It executes the `QUERY4` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `correlation_results` variable.\n",
    "\n",
    "5. Finally, it returns the `correlation_results` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Rent, trees, and complaints correlation\n",
    "# Define the query for correlating rent, trees, and complaints\n",
    "QUERY4 = \"\"\"\n",
    "    WITH RentRank AS (\n",
    "        SELECT zipcode, AVG(rent_price) as average_rent,\n",
    "        RANK() OVER (ORDER BY AVG(rent_price) DESC) rent_rank_desc,\n",
    "        RANK() OVER (ORDER BY AVG(rent_price) ASC) rent_rank_asc\n",
    "        FROM zillow_data\n",
    "        WHERE data_date = '2024-01-31'\n",
    "        GROUP BY zipcode\n",
    "    ),\n",
    "    TreeCount AS (\n",
    "        SELECT zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data\n",
    "        GROUP BY zipcode\n",
    "    ),\n",
    "    ComplaintCount AS (\n",
    "        SELECT zipcode, COUNT(*) as complaint_count\n",
    "        FROM complaints_data\n",
    "        WHERE created_date >= '2024-01-01' AND created_date < '2024-02-01'\n",
    "        GROUP BY zipcode\n",
    "    )\n",
    "    SELECT r.zipcode,\n",
    "    TO_CHAR(r.average_rent, 'FM9,999,999.00') as average_rent,\n",
    "    t.tree_count,\n",
    "    c.complaint_count\n",
    "    FROM RentRank r\n",
    "    JOIN TreeCount t ON r.zipcode = t.zipcode\n",
    "    LEFT JOIN ComplaintCount c ON r.zipcode = c.zipcode\n",
    "    WHERE r.rent_rank_desc <= 5 OR r.rent_rank_asc <= 5\n",
    "    ORDER BY r.average_rent DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file4 = QUERY_DIR / \"rent_trees_complaints_correlation.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file4, \"w\") as file:\n",
    "    file.write(QUERY4)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "correlation_results = read_query(QUERY4, engine)\n",
    "correlation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829b86f",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "1. It defines an SQL query, `QUERY5`, that retrieves the top 10 zipcodes with the most greenery (trees) using a spatial join. The query uses a common table expression (CTE) to:\n",
    "   - Join the `tree_data` and `zipcode_data` tables using a spatial containment operation (`ST_Contains`) to determine which trees belong to each zipcode.\n",
    "   - Count the number of trees for each zipcode.\n",
    "\n",
    "2. It writes the `QUERY5` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "3. It executes the `QUERY5` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `most_greenery_results` variable.\n",
    "\n",
    "4. Finally, it returns the `most_greenery_results` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Most greenery (take 2) using spatial join\n",
    "# Define the query to find the zip codes with the most greenery using a spatial join\n",
    "QUERY5 = \"\"\"\n",
    "    WITH TreeCount AS (\n",
    "        SELECT z.zipcode, COUNT(*) as tree_count\n",
    "        FROM tree_data t\n",
    "        JOIN zipcode_data z ON ST_Contains(z.geom, t.geom)\n",
    "        GROUP BY z.zipcode\n",
    "    )\n",
    "    SELECT zipcode, tree_count\n",
    "    FROM TreeCount\n",
    "    ORDER BY tree_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file5 = QUERY_DIR / \"most_greenery_spatial.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file5, \"w\") as file:\n",
    "    file.write(QUERY5)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "most_greenery_results = read_query(QUERY5, engine)\n",
    "most_greenery_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31864e4",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "1. It defines a central point using the `Point` class from the `shapely.geometry` module, with the coordinates (-73.96253174434912, 40.80737875669467).\n",
    "\n",
    "2. It creates a buffer around the central point with a radius of 0.5 miles (approximately 804.672 meters) using the `buffer()` method of the `Point` object. This creates a circular area around the central point.\n",
    "\n",
    "3. It constructs the SQL query `QUERY6` that retrieves the tree data (tree_id, species, health, status, and location) for all the trees within the buffered area. The `ST_DWithin()` function is used to check if the tree's geometry (`geom`) is within the buffered area.\n",
    "\n",
    "4. It writes the `QUERY6` to a SQL file within the specified `QUERY_DIR` directory. If the directory does not exist, it creates the directory and its parent directories as needed.\n",
    "\n",
    "5. It executes the `QUERY6` using the `read_query()` function, which takes the query and the database engine connection as input, and returns the results as a pandas DataFrame, which is stored in the `trees_within_half_mile` variable.\n",
    "\n",
    "6. Finally, it returns the `trees_within_half_mile` DataFrame, which can be used for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec47e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 6: Trees within  mile radius\n",
    "# Define the central point\n",
    "central_point = Point(-73.96253174434912, 40.80737875669467)\n",
    "\n",
    "# Buffer the central point by 0.5 miles (approximately 804.672 meters)\n",
    "buffered_point = central_point.buffer(0.5 / 69)  # simple degree approximation\n",
    "\n",
    "# Construct the query using the buffered area\n",
    "QUERY6 = f\"\"\"\n",
    "    SELECT tree_id, species, health, status, ST_AsText(geom) as location\n",
    "    FROM tree_data\n",
    "    WHERE ST_DWithin(geom, ST_GeomFromText('{buffered_point.wkt}', 4326), 804.672);\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a SQL file within the specified directory\n",
    "query_file6 = QUERY_DIR / \"trees_within_half_mile.sql\"\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "with open(query_file6, \"w\") as file:\n",
    "    file.write(QUERY6)\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "trees_within_half_mile = read_query(QUERY6, engine)\n",
    "trees_within_half_mile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
